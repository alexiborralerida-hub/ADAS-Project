{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lane Shape Modeling\n",
    "1) load the trained Instance Segmentation model\n",
    "2) predict lane masks\n",
    "3) fit mathematical equations (Polynomials/Splines) to the detected lanes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d853f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import splprep, splev\n",
    "\n",
    "# Configuration\n",
    "BASE_DIR = os.getcwd()\n",
    "DATA_DIR = r'C:\\ADAS_Project\\TUSimple_Small'\n",
    "TRAIN_SET_DIR = os.path.join(DATA_DIR, 'train_set')\n",
    "PROCESSED_DATA_DIR = os.path.join(DATA_DIR, 'processed')\n",
    "CHECKPOINT_DIR = os.path.join(BASE_DIR, 'checkpoints/instance')\n",
    "\n",
    "# polynomial degree to fit\n",
    "degree = 2 \n",
    "\n",
    "# vertical: 30 meters / 288 pixels\n",
    "YM_PER_PIX = 30 / 288 \n",
    "# horizontal: 3.7 meters / 200 pixels (approx lane width in this view)\n",
    "XM_PER_PIX = 3.7 / 200 \n",
    "\n",
    "NUM_CLASSES = 6 \n",
    "IMG_HEIGHT = 288\n",
    "IMG_WIDTH = 512\n",
    "EPOCHS = 5 \n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 5*1e-4\n",
    "model_filename = f'best_model_instance_e{EPOCHS}.pth'\n",
    "BEST_MODEL_PATH = os.path.join(CHECKPOINT_DIR, model_filename)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e310413",
   "metadata": {},
   "source": [
    "## 1. Model & Dataset Definitions\n",
    "Same as lane detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9543fbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TuSimpleDataset(Dataset):\n",
    "    def __init__(self, root_dir, processed_dir, json_files, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.processed_dir = processed_dir\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "\n",
    "        for json_file in json_files:\n",
    "            json_path = os.path.join(root_dir, json_file)\n",
    "            if not os.path.exists(json_path):\n",
    "                continue\n",
    "                \n",
    "            with open(json_path, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "            \n",
    "            for line in lines:\n",
    "                info = json.loads(line)\n",
    "                raw_file = info['raw_file']\n",
    "                mask_file = raw_file.replace('.jpg', '.png')\n",
    "                self.samples.append((raw_file, mask_file))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_rel_path, mask_rel_path = self.samples[idx]\n",
    "        img_path = os.path.join(self.root_dir, img_rel_path)\n",
    "        mask_path = os.path.join(self.processed_dir, mask_rel_path)\n",
    "        \n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        image = cv2.resize(image, (IMG_WIDTH, IMG_HEIGHT))\n",
    "        mask = cv2.resize(mask, (IMG_WIDTH, IMG_HEIGHT), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        image = image.astype(np.float32) / 255.0\n",
    "        image = np.transpose(image, (2, 0, 1))\n",
    "        image = torch.from_numpy(image).float()\n",
    "        mask = torch.from_numpy(mask).long()\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b268f0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # U-Net Architecture\n",
    "# class DoubleConv(nn.Module):\n",
    "#     def __init__(self, in_channels, out_channels):\n",
    "#         super().__init__()\n",
    "#         mid_channels = out_channels\n",
    "#         self.double_conv = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "#             nn.BatchNorm2d(mid_channels),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "#             nn.BatchNorm2d(out_channels),\n",
    "#             nn.ReLU(inplace=True)\n",
    "#         )\n",
    "#     def forward(self, x):\n",
    "#         return self.double_conv(x)\n",
    "\n",
    "# class Down(nn.Module):\n",
    "#     def __init__(self, in_channels, out_channels):\n",
    "#         super().__init__()\n",
    "#         self.maxpool_conv = nn.Sequential(\n",
    "#             nn.MaxPool2d(2),\n",
    "#             DoubleConv(in_channels, out_channels)\n",
    "#         )\n",
    "#     def forward(self, x):\n",
    "#         return self.maxpool_conv(x)\n",
    "\n",
    "# class Up(nn.Module):\n",
    "#     def __init__(self, in_channels, out_channels):\n",
    "#         super().__init__()\n",
    "#         self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "#         self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "#     def forward(self, x1, x2):\n",
    "#         x1 = self.up(x1)\n",
    "        \n",
    "#         diffY = x2.size()[2] - x1.size()[2]\n",
    "#         diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "#         x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "#                         diffY // 2, diffY - diffY // 2])\n",
    "        \n",
    "#         x = torch.cat([x2, x1], dim=1)\n",
    "#         return self.conv(x)\n",
    "\n",
    "# class OutConv(nn.Module):\n",
    "#     def __init__(self, in_channels, out_channels):\n",
    "#         super(OutConv, self).__init__()\n",
    "#         self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "#     def forward(self, x):\n",
    "#         return self.conv(x)\n",
    "\n",
    "# class UNet(nn.Module):\n",
    "#     def __init__(self, n_channels, n_classes):\n",
    "#         super(UNet, self).__init__()\n",
    "#         self.n_channels = n_channels\n",
    "#         self.n_classes = n_classes\n",
    "#         self.inc = DoubleConv(n_channels, 64)\n",
    "#         self.down1 = Down(64, 128)\n",
    "#         self.down2 = Down(128, 256)\n",
    "#         self.down3 = Down(256, 512)\n",
    "#         self.down4 = Down(512, 1024)\n",
    "#         self.up1 = Up(1024, 512)\n",
    "#         self.up2 = Up(512, 256)\n",
    "#         self.up3 = Up(256, 128)\n",
    "#         self.up4 = Up(128, 64)\n",
    "#         self.outc = OutConv(64, n_classes)\n",
    "#     def forward(self, x):\n",
    "#         x1 = self.inc(x)\n",
    "#         x2 = self.down1(x1)\n",
    "#         x3 = self.down2(x2)\n",
    "#         x4 = self.down3(x3)\n",
    "#         x5 = self.down4(x4)\n",
    "#         x = self.up1(x5, x4)\n",
    "#         x = self.up2(x, x3)\n",
    "#         x = self.up3(x, x2)\n",
    "#         x = self.up4(x, x1)\n",
    "#         out = self.outc(x)\n",
    "#         return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b48ccfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#UNet with bilinear\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class Down(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "class Up(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels + in_channels // 2, out_channels)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        \n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        \n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, bilinear=True):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "        self.inc = DoubleConv(n_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = Down(512, 1024 // factor)\n",
    "        self.up1 = Up(1024, 512 // factor, bilinear)\n",
    "        self.up2 = Up(512, 256 // factor, bilinear)\n",
    "        self.up3 = Up(256, 128 // factor, bilinear)\n",
    "        self.up4 = Up(128, 64, bilinear)\n",
    "        self.outc = OutConv(64, n_classes)\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        out = self.outc(x)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c61880",
   "metadata": {},
   "source": [
    "## 2. Curve Fitting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5725f601",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_poly(x, y, degree):\n",
    "    try:\n",
    "        poly_coeffs = np.polyfit(y, x, degree)\n",
    "        poly_func = np.poly1d(poly_coeffs)\n",
    "        return poly_func, poly_coeffs\n",
    "    except np.linalg.LinAlgError:\n",
    "        return None, None\n",
    "\n",
    "def fit_spline(x, y, s=0.0):\n",
    "    \"\"\"Fits a B-spline representation to the lane points.\"\"\"\n",
    "    try:\n",
    "        # Remove duplicates, sort by Y to ensure monotonicity if possible, but splprep handles general parametric\n",
    "        # splrep is for y=f(x), splprep is for parametric (x(t), y(t))\n",
    "        # For lanes, usually monotonic in Y. \n",
    "        \n",
    "        # Let's clean data first: unique points\n",
    "        points = np.column_stack((x, y))\n",
    "        _, unique_indices = np.unique(points, axis=0, return_index=True)\n",
    "        points = points[np.sort(unique_indices)]\n",
    "        \n",
    "        if len(points) < 4:\n",
    "            return None, None\n",
    "            \n",
    "        tck, u = splprep(points.T, u=None, s=s, per=0, k=min(3, len(points)-1))\n",
    "        return tck, u\n",
    "    except Exception as e:\n",
    "        # print(f\"Spline fit failed: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def measure_curvature(y_vals, poly_coeffs):\n",
    "    \"\"\"\n",
    "    Calculates the curvature of the polynomial at the bottom of the image.\n",
    "    Returns radius of curvature in meters.\n",
    "    \"\"\"\n",
    "    # Define y-value where we want radius of curvature (bottom of the image)\n",
    "    y_eval = np.max(y_vals)\n",
    "    \n",
    "    # Recalculate polynomials in world space\n",
    "    # x = ay^2 + by + c\n",
    "    # We need to scale the coefficients to meters\n",
    "    # A_m = A_pix * (xm / ym^2)\n",
    "    # B_m = B_pix * (xm / ym)\n",
    "    A = poly_coeffs[0]\n",
    "    B = poly_coeffs[1]\n",
    "    \n",
    "    A_m = A * (XM_PER_PIX / (YM_PER_PIX ** 2))\n",
    "    B_m = B * (XM_PER_PIX / YM_PER_PIX)\n",
    "    \n",
    "    # R = ((1 + (2Ay + B)^2)^1.5) / |2A|\n",
    "    R_curve = ((1 + (2*A_m*y_eval*YM_PER_PIX + B_m)**2)**1.5) / np.absolute(2*A_m)\n",
    "    return R_curve\n",
    "\n",
    "def measure_offset(poly_coeffs, img_height, img_width):\n",
    "    \"\"\"\n",
    "    Calculates vehicle offset from the center of the lane.\n",
    "    Assumes camera is centered in the car.\n",
    "    \"\"\"\n",
    "    # Calculate x position at the bottom of the image\n",
    "    y_eval = img_height\n",
    "    x_lane_pix = poly_coeffs[0]*y_eval**2 + poly_coeffs[1]*y_eval + poly_coeffs[2]\n",
    "    \n",
    "    # Image center\n",
    "    x_center_pix = img_width / 2\n",
    "    \n",
    "    # Offset in pixels\n",
    "    offset_pix = x_lane_pix - x_center_pix\n",
    "    \n",
    "    # Convert to meters\n",
    "    offset_m = offset_pix * XM_PER_PIX\n",
    "    return offset_m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b02a17",
   "metadata": {},
   "source": [
    "## 3. Execution & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b6afc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_shapes(model_path, data_dir, processed_dir, json_files, num_visualize=5):\n",
    "    dataset = TuSimpleDataset(data_dir, processed_dir, json_files)\n",
    "    loader = DataLoader(dataset, batch_size=1, shuffle=True, num_workers=0)\n",
    "    \n",
    "    model = UNet(n_channels=3, n_classes=NUM_CLASSES).to(device)\n",
    "    if os.path.exists(model_path):\n",
    "        model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "        print(f\"Loaded model from {model_path}\")\n",
    "    else:\n",
    "        print(\"Model checkpoint not found. Please train the model first.\")\n",
    "        return\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, masks in tqdm(loader, desc=\"Fitting Shapes\"):\n",
    "            if count >= num_visualize: break\n",
    "            \n",
    "            images = images.to(device)\n",
    "            # masks = masks.to(device) # We rely on prediction\n",
    "            \n",
    "            output = model(images)\n",
    "            probs = torch.softmax(output, dim=1)\n",
    "            pred_mask = torch.argmax(probs, dim=1).squeeze(0).cpu().numpy()\n",
    "            \n",
    "            # Visualization Setup\n",
    "            img_np = images[0].cpu().permute(1, 2, 0).numpy()\n",
    "            img_np = (img_np * 255).astype(np.uint8)\n",
    "            img_vis = img_np.copy()\n",
    "            img_vis = cv2.cvtColor(img_vis, cv2.COLOR_RGB2BGR)\n",
    "            \n",
    "            # Color Palette for lanes\n",
    "            colors = [(0, 255, 0), (255, 0, 0), (0, 0, 255), (255, 255, 0), (255, 0, 255)]\n",
    "            \n",
    "            # Iterate over each lane class\n",
    "            for cls_id in range(1, NUM_CLASSES):\n",
    "                # Extract pixels for this class\n",
    "                y_idx, x_idx = np.where(pred_mask == cls_id)\n",
    "                \n",
    "                if len(y_idx) < 50: continue # Skip if too few points\n",
    "                \n",
    "                color = colors[(cls_id - 1) % len(colors)]\n",
    "                \n",
    "                # --- Method 1: Polynomial Fit (x = ay^2 + by + c) ---\n",
    "                poly_func, coeffs = fit_poly(x_idx, y_idx, degree)\n",
    "                if poly_func:\n",
    "                    # Generate points for drawing\n",
    "                    plot_y = np.linspace(min(y_idx), max(y_idx), 100)\n",
    "                    plot_x = poly_func(plot_y)\n",
    "                    \n",
    "                    pts = np.array([np.transpose(np.vstack([plot_x, plot_y]))]).astype(np.int32)\n",
    "                    # Draw Poly Line\n",
    "                    cv2.polylines(img_vis, pts, isClosed=False, color=color, thickness=2)\n",
    "                    \n",
    "                    # Add Equation Text (Example for first lane)\n",
    "                    # cv2.putText(img_vis, f\"Lane {cls_id}: Poly\", (int(plot_x[0]), int(plot_y[0])), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)\n",
    "\n",
    "                    # --- Measure Curvature & Offset ---\n",
    "                    r_curve = measure_curvature(plot_y, coeffs)\n",
    "                    offset = measure_offset(coeffs, IMG_HEIGHT, IMG_WIDTH)\n",
    "                    \n",
    "                    # Display Metrics\n",
    "                    label = f\"L{cls_id}: R={r_curve:.0f}m Off={offset:.2f}m\"\n",
    "                    cv2.putText(img_vis, label, (int(plot_x[-1])-50, int(plot_y[-1])-10), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.4, color, 1, cv2.LINE_AA)\n",
    "\n",
    "                # --- Method 2: Spline Fit (Optional, commented out to avoid clutter) ---\n",
    "                # tck, u = fit_spline(x_idx, y_idx)\n",
    "                # if tck:\n",
    "                #     new_points = splev(np.linspace(0, 1, 100), tck)\n",
    "                #     pts_spline = np.array([np.transpose(np.vstack(new_points))]).astype(np.int32)\n",
    "                #     cv2.polylines(img_vis, pts_spline, isClosed=False, color=(255, 255, 255), thickness=1) # White for spline\n",
    "\n",
    "           \n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.imshow(cv2.cvtColor(img_vis, cv2.COLOR_BGR2RGB))\n",
    "            plt.title(f\"Lane Curve Fitting (Poly Degree 2)\")\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "            \n",
    "            count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd3cfbc6",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for UNet:\n\tMissing key(s) in state_dict: \"up1.up.weight\", \"up1.up.bias\", \"up2.up.weight\", \"up2.up.bias\", \"up3.up.weight\", \"up3.up.bias\", \"up4.up.weight\", \"up4.up.bias\". \n\tsize mismatch for down4.maxpool_conv.1.double_conv.0.weight: copying a param with shape torch.Size([512, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([1024, 512, 3, 3]).\n\tsize mismatch for down4.maxpool_conv.1.double_conv.1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for down4.maxpool_conv.1.double_conv.1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for down4.maxpool_conv.1.double_conv.1.running_mean: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for down4.maxpool_conv.1.double_conv.1.running_var: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for down4.maxpool_conv.1.double_conv.3.weight: copying a param with shape torch.Size([512, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([1024, 1024, 3, 3]).\n\tsize mismatch for down4.maxpool_conv.1.double_conv.4.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for down4.maxpool_conv.1.double_conv.4.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for down4.maxpool_conv.1.double_conv.4.running_mean: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for down4.maxpool_conv.1.double_conv.4.running_var: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for up1.conv.double_conv.3.weight: copying a param with shape torch.Size([256, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 512, 3, 3]).\n\tsize mismatch for up1.conv.double_conv.4.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for up1.conv.double_conv.4.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for up1.conv.double_conv.4.running_mean: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for up1.conv.double_conv.4.running_var: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for up2.conv.double_conv.3.weight: copying a param with shape torch.Size([128, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3]).\n\tsize mismatch for up2.conv.double_conv.4.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for up2.conv.double_conv.4.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for up2.conv.double_conv.4.running_mean: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for up2.conv.double_conv.4.running_var: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for up3.conv.double_conv.3.weight: copying a param with shape torch.Size([64, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n\tsize mismatch for up3.conv.double_conv.4.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for up3.conv.double_conv.4.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for up3.conv.double_conv.4.running_mean: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for up3.conv.double_conv.4.running_var: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for outc.conv.weight: copying a param with shape torch.Size([2, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([6, 64, 1, 1]).\n\tsize mismatch for outc.conv.bias: copying a param with shape torch.Size([2]) from checkpoint, the shape in current model is torch.Size([6]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Run\u001b[39;00m\n\u001b[0;32m      2\u001b[0m json_files \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel_data_0313.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel_data_0531.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel_data_0601.json\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 3\u001b[0m \u001b[43mevaluate_shapes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mADAS_Project\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mADAS-Project\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mcheckpoints\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mbest_model_e5_bs8_lr0.0005.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTRAIN_SET_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPROCESSED_DATA_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson_files\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 7\u001b[0m, in \u001b[0;36mevaluate_shapes\u001b[1;34m(model_path, data_dir, processed_dir, json_files, num_visualize)\u001b[0m\n\u001b[0;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m UNet(n_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, n_classes\u001b[38;5;241m=\u001b[39mNUM_CLASSES)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(model_path):\n\u001b[1;32m----> 7\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded model from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\orcun\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2581\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2573\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2574\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   2575\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2576\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[0;32m   2577\u001b[0m             ),\n\u001b[0;32m   2578\u001b[0m         )\n\u001b[0;32m   2580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2581\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   2582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2583\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[0;32m   2584\u001b[0m         )\n\u001b[0;32m   2585\u001b[0m     )\n\u001b[0;32m   2586\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for UNet:\n\tMissing key(s) in state_dict: \"up1.up.weight\", \"up1.up.bias\", \"up2.up.weight\", \"up2.up.bias\", \"up3.up.weight\", \"up3.up.bias\", \"up4.up.weight\", \"up4.up.bias\". \n\tsize mismatch for down4.maxpool_conv.1.double_conv.0.weight: copying a param with shape torch.Size([512, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([1024, 512, 3, 3]).\n\tsize mismatch for down4.maxpool_conv.1.double_conv.1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for down4.maxpool_conv.1.double_conv.1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for down4.maxpool_conv.1.double_conv.1.running_mean: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for down4.maxpool_conv.1.double_conv.1.running_var: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for down4.maxpool_conv.1.double_conv.3.weight: copying a param with shape torch.Size([512, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([1024, 1024, 3, 3]).\n\tsize mismatch for down4.maxpool_conv.1.double_conv.4.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for down4.maxpool_conv.1.double_conv.4.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for down4.maxpool_conv.1.double_conv.4.running_mean: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for down4.maxpool_conv.1.double_conv.4.running_var: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for up1.conv.double_conv.3.weight: copying a param with shape torch.Size([256, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 512, 3, 3]).\n\tsize mismatch for up1.conv.double_conv.4.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for up1.conv.double_conv.4.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for up1.conv.double_conv.4.running_mean: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for up1.conv.double_conv.4.running_var: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for up2.conv.double_conv.3.weight: copying a param with shape torch.Size([128, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3]).\n\tsize mismatch for up2.conv.double_conv.4.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for up2.conv.double_conv.4.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for up2.conv.double_conv.4.running_mean: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for up2.conv.double_conv.4.running_var: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for up3.conv.double_conv.3.weight: copying a param with shape torch.Size([64, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n\tsize mismatch for up3.conv.double_conv.4.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for up3.conv.double_conv.4.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for up3.conv.double_conv.4.running_mean: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for up3.conv.double_conv.4.running_var: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for outc.conv.weight: copying a param with shape torch.Size([2, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([6, 64, 1, 1]).\n\tsize mismatch for outc.conv.bias: copying a param with shape torch.Size([2]) from checkpoint, the shape in current model is torch.Size([6])."
     ]
    }
   ],
   "source": [
    "\n",
    "# Run\n",
    "json_files = ['label_data_0313.json', 'label_data_0531.json', 'label_data_0601.json']\n",
    "evaluate_shapes(r'C:\\ADAS_Project\\ADAS-Project\\checkpoints\\best_model_e5_bs8_lr0.0005.pth', TRAIN_SET_DIR, PROCESSED_DATA_DIR, json_files)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
