{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lane Detection with TuSimple Dataset\n",
    "Using U-Net\n",
    "1. Configuration\n",
    "2. Data Preprocessing (JSON to Masks)\n",
    "3. Dataset Loading\n",
    "4. Model Architecture (U-Net)\n",
    "5. Training Loop\n",
    "6. Inference & Visualization\n",
    "7. Execution\n",
    "8. Evaluation\n",
    "\n",
    "Save the Confusion Matrix while the validation\n",
    "\n",
    "differentiation of lanes: arc loss, distance between lanes. \n",
    "last layer \n",
    "instance segmentation\n",
    "\n",
    "+3 credits: from instance segmentation -> shape of lane markings scipy make_splprep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration\n",
    "Define paths, hyperparameters, and device settings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "BASE_DIR = os.getcwd()\n",
    "DATA_DIR = r'C:\\ADAS_Project\\TUSimple_Small' \n",
    "TRAIN_SET_DIR = os.path.join(DATA_DIR, 'train_set')\n",
    "TEST_SET_DIR = os.path.join(DATA_DIR, 'test_set')\n",
    "PROCESSED_DATA_DIR = os.path.join(DATA_DIR, r'processed\\instance')\n",
    "CHECKPOINT_DIR = os.path.join(BASE_DIR, r'checkpoints\\instance')\n",
    "\n",
    "os.makedirs(PROCESSED_DATA_DIR, exist_ok=True)\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "# Hyperparameters\n",
    "EPOCHS = 20 #20+ for final training\n",
    "BATCH_SIZE = 16 # calculate based on GPU memory \n",
    "LEARNING_RATE = 5*1e-4 \n",
    "WEIGHT_DECAY = 1e-4 \n",
    "NUM_CLASSES = 6  # background + all lanes \n",
    "IMG_HEIGHT = 288 # resize target height\n",
    "IMG_WIDTH = 512  # resize target width\n",
    "\n",
    "# save model checkpoints\n",
    "model_filename = f'best_model_instance_e{EPOCHS}_dropout.pth'\n",
    "BEST_MODEL_PATH = os.path.join(CHECKPOINT_DIR, model_filename)\n",
    "eval_filename = f'best_model_instance_e{EPOCHS}_dropout.pth'\n",
    "EVAL_MODEL_PATH = os.path.join(CHECKPOINT_DIR, eval_filename)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing\n",
    "Functions to parse TuSimple JSON labels and generate segmentation masks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tusimple_data(data_dir, output_dir, json_files):\n",
    "    \n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    for json_file in json_files:\n",
    "        json_path = os.path.join(data_dir, json_file)\n",
    "\n",
    "        with open(json_path, 'r') as openfile: #read json file \n",
    "            lines = openfile.readlines()\n",
    "\n",
    "        for line in tqdm(lines, desc=f\"Processing {json_file}\"):\n",
    "            info = json.loads(line)\n",
    "            raw_file = info['raw_file']\n",
    "            lanes = info['lanes']\n",
    "            h_samples = info['h_samples']\n",
    "            \n",
    "            # create mask assuming 1280x720\n",
    "            mask = np.zeros((720, 1280), dtype=np.uint8)\n",
    "\n",
    "            # lanes[0] -> Class 1\n",
    "            # lanes[1] -> Class 2\n",
    "            for i, lane in enumerate(lanes):\n",
    "                # Skip if lane is all -2 (empty)\n",
    "                if all(x == -2 for x in lane):\n",
    "                    continue\n",
    "                    \n",
    "                points = []\n",
    "                for x, y in zip(lane, h_samples):\n",
    "                    if x != -2:\n",
    "                        points.append((x, y))\n",
    "                \n",
    "                if len(points) > 1:\n",
    "                    class_id = i + 1\n",
    "                    cv2.polylines(mask, [np.array(points, dtype=np.int32)], isClosed=False, color=class_id, thickness=10) \n",
    "                    \n",
    "            # save mask\n",
    "            mask_rel_path = raw_file.replace('.jpg', '.png')\n",
    "            mask_save_path = os.path.join(output_dir, mask_rel_path)\n",
    "            \n",
    "            os.makedirs(os.path.dirname(mask_save_path), exist_ok=True)\n",
    "            cv2.imwrite(mask_save_path, mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Definition\n",
    "PyTorch Dataset class to load images and masks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TuSimpleDataset(Dataset):\n",
    "    def __init__(self, root_dir, processed_dir, json_files, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.processed_dir = processed_dir\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "\n",
    "        # load all samples from json files\n",
    "        for json_file in json_files:\n",
    "            json_path = os.path.join(root_dir, json_file)\n",
    "            if not os.path.exists(json_path):\n",
    "                continue\n",
    "                \n",
    "            with open(json_path, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "            \n",
    "            for line in lines:\n",
    "                info = json.loads(line)\n",
    "                raw_file = info['raw_file']\n",
    "                mask_file = raw_file.replace('.jpg', '.png')\n",
    "                \n",
    "                self.samples.append((raw_file, mask_file))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_rel_path, mask_rel_path = self.samples[idx]\n",
    "        \n",
    "        img_path = os.path.join(self.root_dir, img_rel_path)\n",
    "        mask_path = os.path.join(self.processed_dir, mask_rel_path)\n",
    "        \n",
    "        # load image and mask\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        # resize and normalize\n",
    "        image = cv2.resize(image, (IMG_WIDTH, IMG_HEIGHT))\n",
    "        mask = cv2.resize(mask, (IMG_WIDTH, IMG_HEIGHT), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        image = image.astype(np.float32) / 255.0\n",
    "        image = np.transpose(image, (2, 0, 1)) # channel first\n",
    "      \n",
    "        image = torch.from_numpy(image).float()\n",
    "        mask = torch.from_numpy(mask).long()\n",
    "        \n",
    "        return image, mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Architecture (U-Net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    #(convolution => [BN] => ReLU) * 2\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        # padding=1\n",
    "        mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.2) #may be used for overfitting of lane 4-5\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder - downsampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Down(nn.Module):\n",
    "    #downscaling with maxpool then double conv    \n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        # 256x256 -> 128x128\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoder - upsampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Up(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        # upsample  \n",
    "        self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "        self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        # x1 previous layer \n",
    "        # x2 corresponding encoder layer (skip connection)\n",
    "        x1 = self.up(x1)\n",
    "        \n",
    "        # calculate the difference in size between x1 (upsampled) and x2 (skip connection)\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "        \n",
    "        # Pad x1 to match the size of x2\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        \n",
    "        # skip connection\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        # 1x1 convolution to map feature channels to the number of classes\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module): \n",
    "    #full unet\n",
    "    def __init__(self, n_channels, n_classes):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        # define encoder sizes\n",
    "        self.inc = DoubleConv(n_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        self.down4 = Down(512, 1024)\n",
    "        # define decoder sizes\n",
    "        self.up1 = Up(1024, 512)\n",
    "        self.up2 = Up(512, 256)\n",
    "        self.up3 = Up(256, 128)\n",
    "        self.up4 = Up(128, 64)\n",
    "        self.outc = OutConv(64, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # encoder \n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        \n",
    "        # decoder - with skip connections\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "\n",
    "        # final\n",
    "        out = self.outc(x) \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    # dataset\n",
    "    json_files = ['label_data_0313.json', 'label_data_0531.json', 'label_data_0601.json']\n",
    "    \n",
    "    full_dataset = TuSimpleDataset(TRAIN_SET_DIR, PROCESSED_DATA_DIR, json_files)\n",
    "\n",
    "    # split into train and val\n",
    "    train_size = int(0.9 * len(full_dataset))\n",
    "    val_size = len(full_dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "    # model\n",
    "    model = UNet(n_channels=3, n_classes=NUM_CLASSES).to(device) # 3 channels for R,G,B \n",
    "\n",
    "    # loss and optimizer (instance seg -> adding other lane weights)\n",
    "    class_weights = torch.tensor([0.4, 1.0, 1.0, 1.0, 1.0, 1.0], device=device) # background, lanes\n",
    "    #changed the background weight from 0.2 to 0.4 to further reduce excessive noise/blobs\n",
    "    criterion_ce = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "\n",
    "    best_val_loss = float('inf') # checkpoint\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\") #tqdm is a progress bar\n",
    "        \n",
    "        for images, masks in loop:\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "\n",
    "            # forward pass\n",
    "            outputs = model(images)\n",
    "\n",
    "            loss_ce = criterion_ce(outputs, masks)\n",
    "            loss = loss_ce\n",
    "\n",
    "            # backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            loop.set_postfix(loss=loss.item(), ce=loss_ce.item())\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        \n",
    "        # validation, no_grad for validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "\n",
    "        # Initialize Confusion Matrix (Ground Truth x Prediction)\n",
    "        conf_matrix = np.zeros((NUM_CLASSES, NUM_CLASSES), dtype=np.int64)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, masks in val_loader:\n",
    "                images = images.to(device)\n",
    "                masks = masks.to(device)\n",
    "                outputs = model(images)\n",
    "                \n",
    "                loss_ce = criterion_ce(outputs, masks)\n",
    "                loss = loss_ce\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "\n",
    "                # Metrics Calculation\n",
    "                probs = torch.softmax(outputs, dim=1)\n",
    "                preds = torch.argmax(probs, dim=1) # [Batch, H, W]\n",
    "                \n",
    "                # Move to CPU and flatten\n",
    "                preds_flat = preds.view(-1).cpu().numpy()\n",
    "                masks_flat = masks.view(-1).cpu().numpy()\n",
    "                \n",
    "                # Efficient Confusion Matrix Update\n",
    "                # bins = NUM_CLASSES * true_label + pred_label\n",
    "                bins = NUM_CLASSES * masks_flat + preds_flat\n",
    "                bincount = np.bincount(bins, minlength=NUM_CLASSES**2)\n",
    "                conf_matrix += bincount.reshape(NUM_CLASSES, NUM_CLASSES)\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{EPOCHS}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        \n",
    "        # Save Confusion Matrix\n",
    "        cm_filename = f\"confusion_matrix_e{EPOCHS}.npy\"\n",
    "        cm_path = os.path.join(CHECKPOINT_DIR, cm_filename)\n",
    "        np.save(cm_path, conf_matrix)\n",
    "        print(f\"Saved confusion matrix\")\n",
    "\n",
    "\n",
    "        # save best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), BEST_MODEL_PATH)\n",
    "            print(\"Saved best model checkpoint.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Inference & Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lane_detection_prediction(image_path, model_path, output_path):\n",
    "    # load model\n",
    "    model = UNet(n_channels=3, n_classes=NUM_CLASSES).to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    # load image\n",
    "    example_image = cv2.imread(image_path)\n",
    "    \n",
    "    image = cv2.cvtColor(example_image, cv2.COLOR_BGR2RGB) #convert to RGB\n",
    "    image = cv2.resize(image, (IMG_WIDTH, IMG_HEIGHT)) #resize to match input size\n",
    "    image = image.astype(np.float32) / 255.0 #normalize\n",
    "    image = np.transpose(image, (2, 0, 1)) #channels first\n",
    "    image = torch.from_numpy(image).float().unsqueeze(0).to(device) \n",
    "\n",
    "    # inference\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        probs = torch.softmax(output, dim=1)\n",
    "        pred_mask = torch.argmax(probs, dim=1).squeeze(0).cpu().numpy()\n",
    "    \n",
    "    # visualization\n",
    "    pred_mask_resized = cv2.resize(pred_mask.astype(np.uint8), (example_image.shape[1], example_image.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "    \n",
    "    #for binary seg -> five lane classes\n",
    "    colors = [\n",
    "        [0, 0, 0],       # Background\n",
    "        [0, 255, 0],     # Lane 1 (Green)\n",
    "        [255, 0, 0],     # Lane 2 (Red)\n",
    "        [0, 0, 255],     # Lane 3 (Blue)\n",
    "        [255, 255, 0],   # Lane 4 (Yellow)\n",
    "        [255, 0, 255]    # Lane 5 (Magenta)\n",
    "    ]\n",
    "    \n",
    "    #blob filtering\n",
    "    overlay = np.zeros_like(example_image)\n",
    "    for i in range(1, NUM_CLASSES):\n",
    "    \n",
    "        class_mask = (pred_mask_resized == i).astype(np.uint8)\n",
    "        num_labels, labels_im, stats, centroids = cv2.connectedComponentsWithStats(class_mask)\n",
    "        if num_labels > 1:\n",
    "            largest_rel_idx = np.argmax(stats[1:, cv2.CC_STAT_AREA])\n",
    "            largest_comp_idx = largest_rel_idx + 1\n",
    "            max_area = stats[largest_comp_idx, cv2.CC_STAT_AREA]\n",
    "            max_height = stats[largest_comp_idx, cv2.CC_STAT_HEIGHT]\n",
    "            if max_area >= 1000 and max_height >= 50:\n",
    "                valid_mask = (labels_im == largest_comp_idx)\n",
    "                overlay[valid_mask] = colors[i]\n",
    "   \n",
    "\n",
    "    result = cv2.addWeighted(example_image, 1, overlay, 0.5, 0) #overlay mask on original image\n",
    "\n",
    "    cv2.imwrite(output_path, result)\n",
    "    print(f\"Saved result to {output_path}\")\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(cv2.cvtColor(result, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Execution\n",
    "Execution of the training functions and inference. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Run Preprocessing\n",
    "json_files = ['label_data_0313.json', 'label_data_0531.json', 'label_data_0601.json']\n",
    "process_tusimple_data(TRAIN_SET_DIR, PROCESSED_DATA_DIR, json_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Train Model\n",
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Inference\n",
    "test_image_path1 = os.path.join(TRAIN_SET_DIR, 'clips/0313-2/50/10.jpg') # Example image\n",
    "test_image_path2 = os.path.join(TRAIN_SET_DIR, 'clips/0313-1/1800/10.jpg') \n",
    "test_image_path3 = os.path.join(TRAIN_SET_DIR, 'clips/0601/1494452553518276564/10.jpg') \n",
    "lane_detection_prediction(test_image_path1, BEST_MODEL_PATH, 'instance_result.jpg')\n",
    "lane_detection_prediction(test_image_path2, BEST_MODEL_PATH, 'instance_result2.jpg')\n",
    "lane_detection_prediction(test_image_path3, BEST_MODEL_PATH, 'instance_result3.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluation\n",
    "Calculate IoU and Accuracy on the Test Set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(pred_mask, true_mask, num_classes):\n",
    "    # pred_mask, true_mask: [H, W]\n",
    "    iou_per_class = []\n",
    "    \n",
    "    for classes in range(num_classes): \n",
    "        pred_classes = (pred_mask == classes)\n",
    "        true_classes = (true_mask == classes)\n",
    "        \n",
    "        intersection = np.logical_and(pred_classes, true_classes).sum()\n",
    "        union = np.logical_or(pred_classes, true_classes).sum()\n",
    "        \n",
    "        if union == 0:\n",
    "            iou = np.nan # Handle division by zero if class is not present in both\n",
    "        else:\n",
    "            iou = intersection / union\n",
    "          \n",
    "        iou_per_class.append(iou)\n",
    "        \n",
    "    return iou_per_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tolerance_iou(pred_mask, true_mask, num_classes, tolerance=5):\n",
    "    # Tolerance IoU: Dilate GT to allow slack\n",
    "    iou_per_class = []\n",
    "    kernel = np.ones((tolerance, tolerance), np.uint8)\n",
    "    \n",
    "    for cls in range(num_classes):\n",
    "        if cls == 0: #strict for background usually, or skip\n",
    "            # Standard IoU for background\n",
    "            p = (pred_mask == cls)\n",
    "            t = (true_mask == cls)\n",
    "            inter = np.logical_and(p, t).sum()\n",
    "            union = np.logical_or(p, t).sum()\n",
    "            iou_per_class.append(inter / union if union > 0 else np.nan)\n",
    "            continue\n",
    "\n",
    "        pred_cls = (pred_mask == cls).astype(np.uint8)\n",
    "        true_cls = (true_mask == cls).astype(np.uint8)\n",
    "        \n",
    "        # Dilate Ground Truth\n",
    "        dilated_true = cv2.dilate(true_cls, kernel, iterations=1)\n",
    "        \n",
    "        intersection = np.logical_and(pred_cls, dilated_true).sum()\n",
    "        union = np.logical_or(pred_cls, true_cls).sum()\n",
    "        \n",
    "        if union == 0:\n",
    "            iou_per_class.append(np.nan)\n",
    "        else:\n",
    "            iou_per_class.append(intersection / union)\n",
    "            \n",
    "    return iou_per_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader, device):\n",
    "    num_visualize = 3 \n",
    "    model.eval()\n",
    "    \n",
    "    total_iou = []\n",
    "    total_tol_iou = []\n",
    "    conf_matrix = np.zeros((NUM_CLASSES, NUM_CLASSES), dtype=np.int64)\n",
    "    viz_samples = []\n",
    "    \n",
    "    colors = [\n",
    "        [0, 0, 0], [0, 255, 0], [255, 0, 0], [0, 0, 255], [255, 255, 0], [255, 0, 255]\n",
    "    ]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, masks in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            images = images.to(device)\n",
    "            masks_np = masks.cpu().numpy()\n",
    "            masks = masks.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            preds = torch.argmax(probs, dim=1).cpu().numpy()\n",
    "            \n",
    "            for i in range(len(masks_np)):\n",
    "                # 1. Standard IoU\n",
    "                iou = calculate_iou(preds[i], masks_np[i], NUM_CLASSES)\n",
    "                total_iou.append(iou)\n",
    "                \n",
    "                # 2. Tolerance IoU\n",
    "                tol_iou = calculate_tolerance_iou(preds[i], masks_np[i], NUM_CLASSES, tolerance=5)\n",
    "                total_tol_iou.append(tol_iou)\n",
    "                \n",
    "                # 3. Confusion Matrix Update\n",
    "                p_flat = preds[i].flatten()\n",
    "                t_flat = masks_np[i].flatten()\n",
    "                bins = NUM_CLASSES * t_flat + p_flat\n",
    "                bincount = np.bincount(bins, minlength=NUM_CLASSES**2)\n",
    "                conf_matrix += bincount.reshape(NUM_CLASSES, NUM_CLASSES)\n",
    "\n",
    "                # Visualization Collection\n",
    "                if len(viz_samples) < num_visualize:\n",
    "                    img_np = images[i].cpu().permute(1, 2, 0).numpy()\n",
    "                    img_np = (img_np * 255).astype(np.uint8)\n",
    "                    img_np_rgb = cv2.cvtColor(img_np, cv2.COLOR_BGR2RGB)\n",
    "                    mean_lane_iou = np.nanmean(iou[1:])\n",
    "                    viz_samples.append((img_np_rgb, masks_np[i], preds[i], mean_lane_iou))\n",
    " \n",
    "    #Metrics\n",
    "    mean_iou = np.nanmean(total_iou, axis=0)\n",
    "    mean_tol_iou = np.nanmean(total_tol_iou, axis=0)\n",
    "    \n",
    "    for i in range(NUM_CLASSES):\n",
    "        cls_name = \"Background\" if i == 0 else f\"Lane {i}\"\n",
    "        tp = conf_matrix[i, i]\n",
    "        fp = conf_matrix[:, i].sum() - tp\n",
    "        fn = conf_matrix[i, :].sum() - tp\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        \n",
    "        print(f\"{cls_name}:\")\n",
    "        print(f\"  Precision: {precision:.4f}, Recall: {recall:.4f}\")\n",
    "        print(f\"  Std IoU:   {mean_iou[i]:.4f}\")\n",
    "        print(f\"  Tol IoU:   {mean_tol_iou[i]:.4f}\")\n",
    "    \n",
    "    print(f\"\\nMean Std IoU (Lanes): {np.nanmean(mean_iou[1:]):.4f}\")\n",
    "    print(f\"Mean Tol IoU (Lanes): {np.nanmean(mean_tol_iou[1:]):.4f}\")\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(num_visualize, 3, figsize=(15, 5 * num_visualize))\n",
    "    colors = [[0,0,0], [0,255,0], [255,0,0], [0,0,255], [255,255,0], [255,0,255]]\n",
    "    for idx, (img, gt, pred, iou) in enumerate(viz_samples):\n",
    "        current_axes = axes[idx]\n",
    "        current_axes[0].imshow(img)\n",
    "        current_axes[0].set_title(\"Input\")\n",
    "        current_axes[0].axis('off')\n",
    "        \n",
    "        ov_gt = img.copy()\n",
    "        ov_pred = img.copy()\n",
    "        for c in range(1, NUM_CLASSES):\n",
    "             ov_gt[gt==c] = colors[c]\n",
    "             ov_pred[pred==c] = colors[c]\n",
    "             \n",
    "        current_axes[1].imshow(ov_gt)\n",
    "        current_axes[1].set_title(\"Ground Truth\")\n",
    "        current_axes[1].axis('off')\n",
    "        \n",
    "        current_axes[2].imshow(ov_pred)\n",
    "        current_axes[2].set_title(f\"Pred (IoU: {iou:.2f})\")\n",
    "        current_axes[2].axis('off')\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_json_files = ['test_label.json']\n",
    "process_tusimple_data(TEST_SET_DIR, PROCESSED_DATA_DIR, test_json_files)\n",
    "\n",
    "test_dataset = TuSimpleDataset(TEST_SET_DIR, PROCESSED_DATA_DIR, test_json_files)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "model = UNet(n_channels=3, n_classes=NUM_CLASSES).to(device) #3 channels for rgb images\n",
    "model.load_state_dict(torch.load(EVAL_MODEL_PATH, map_location=device))\n",
    "\n",
    "evaluate(model, test_loader, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
