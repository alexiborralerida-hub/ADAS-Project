{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lane Detection with TuSimple Dataset\n",
    "Using U-Net\n",
    "1. Configuration\n",
    "2. Data Preprocessing (JSON to Masks)\n",
    "3. Dataset Loading\n",
    "4. Model Architecture (U-Net)\n",
    "5. Training Loop\n",
    "6. Inference & Visualization\n",
    "7. Execution\n",
    "8. Evaluation\n",
    "\n",
    "Save the Confusion Matrix while the validation\n",
    "\n",
    "differentiation of lanes: arc loss, distance between lanes. \n",
    "last layer \n",
    "instance segmentation\n",
    "\n",
    "+3 credits: from instance segmentation -> shape of lane markings scipy make_splprep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration\n",
    "Define paths, hyperparameters, and device settings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "BASE_DIR = os.getcwd()\n",
    "DATA_DIR = r'C:\\ADAS_Project\\TUSimple_Small' \n",
    "TRAIN_SET_DIR = os.path.join(DATA_DIR, 'train_set')\n",
    "TEST_SET_DIR = os.path.join(DATA_DIR, 'test_set')\n",
    "PROCESSED_DATA_DIR = os.path.join(DATA_DIR, r'processed\\instance')\n",
    "CHECKPOINT_DIR = os.path.join(BASE_DIR, r'checkpoints\\instance')\n",
    "\n",
    "os.makedirs(PROCESSED_DATA_DIR, exist_ok=True)\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "# Hyperparameters\n",
    "EPOCHS = 20 #20+ for final training\n",
    "BATCH_SIZE = 8 # calculate based on GPU memory \n",
    "LEARNING_RATE = 5*1e-4 \n",
    "WEIGHT_DECAY = 1e-4 \n",
    "NUM_CLASSES = 6  # background + all lanes \n",
    "IMG_HEIGHT = 288 # resize target height\n",
    "IMG_WIDTH = 512  # resize target width\n",
    "\n",
    "# save model checkpoints\n",
    "model_filename = f'best_model_instance_e{EPOCHS}_dropout.pth'\n",
    "BEST_MODEL_PATH = os.path.join(CHECKPOINT_DIR, model_filename)\n",
    "eval_filename = f'best_model_instance_e{EPOCHS}_dropout.pth'\n",
    "EVAL_MODEL_PATH = os.path.join(CHECKPOINT_DIR, eval_filename)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing\n",
    "Functions to parse TuSimple JSON labels and generate segmentation masks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tusimple_data(data_dir, output_dir, json_files):\n",
    "    \n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    for json_file in json_files:\n",
    "        json_path = os.path.join(data_dir, json_file)\n",
    "\n",
    "        with open(json_path, 'r') as openfile: #read json file \n",
    "            lines = openfile.readlines()\n",
    "\n",
    "        for line in tqdm(lines, desc=f\"Processing {json_file}\"):\n",
    "            info = json.loads(line)\n",
    "            raw_file = info['raw_file']\n",
    "            lanes = info['lanes']\n",
    "            h_samples = info['h_samples']\n",
    "            \n",
    "            # create mask assuming 1280x720\n",
    "            mask = np.zeros((720, 1280), dtype=np.uint8)\n",
    "\n",
    "            # lanes[0] -> Class 1\n",
    "            # lanes[1] -> Class 2\n",
    "            for i, lane in enumerate(lanes):\n",
    "                # Skip if lane is all -2 (empty)\n",
    "                if all(x == -2 for x in lane):\n",
    "                    continue\n",
    "                    \n",
    "                points = []\n",
    "                for x, y in zip(lane, h_samples):\n",
    "                    if x != -2:\n",
    "                        points.append((x, y))\n",
    "                \n",
    "                if len(points) > 1:\n",
    "                    class_id = i + 1\n",
    "                    cv2.polylines(mask, [np.array(points, dtype=np.int32)], isClosed=False, color=class_id, thickness=10) \n",
    "                    \n",
    "            # save mask\n",
    "            mask_rel_path = raw_file.replace('.jpg', '.png')\n",
    "            mask_save_path = os.path.join(output_dir, mask_rel_path)\n",
    "            \n",
    "            os.makedirs(os.path.dirname(mask_save_path), exist_ok=True)\n",
    "            cv2.imwrite(mask_save_path, mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Definition\n",
    "PyTorch Dataset class to load images and masks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TuSimpleDataset(Dataset):\n",
    "    def __init__(self, root_dir, processed_dir, json_files, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.processed_dir = processed_dir\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "\n",
    "        # load all samples from json files\n",
    "        for json_file in json_files:\n",
    "            json_path = os.path.join(root_dir, json_file)\n",
    "            if not os.path.exists(json_path):\n",
    "                continue\n",
    "                \n",
    "            with open(json_path, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "            \n",
    "            for line in lines:\n",
    "                info = json.loads(line)\n",
    "                raw_file = info['raw_file']\n",
    "                mask_file = raw_file.replace('.jpg', '.png')\n",
    "                \n",
    "                self.samples.append((raw_file, mask_file))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_rel_path, mask_rel_path = self.samples[idx]\n",
    "        \n",
    "        img_path = os.path.join(self.root_dir, img_rel_path)\n",
    "        mask_path = os.path.join(self.processed_dir, mask_rel_path)\n",
    "        \n",
    "        # load image and mask\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        # resize and normalize\n",
    "        image = cv2.resize(image, (IMG_WIDTH, IMG_HEIGHT))\n",
    "        mask = cv2.resize(mask, (IMG_WIDTH, IMG_HEIGHT), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        image = image.astype(np.float32) / 255.0\n",
    "        image = np.transpose(image, (2, 0, 1)) # channel first\n",
    "      \n",
    "        image = torch.from_numpy(image).float()\n",
    "        mask = torch.from_numpy(mask).long()\n",
    "        \n",
    "        return image, mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Architecture (U-Net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    #(convolution => [BN] => ReLU) * 2\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        # padding=1\n",
    "        mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.2) #may be used for overfitting of lane 4-5\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder - downsampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Down(nn.Module):\n",
    "    #downscaling with maxpool then double conv    \n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        # 256x256 -> 128x128\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoder - upsampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Up(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        # upsample  \n",
    "        self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "        self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        # x1 previous layer \n",
    "        # x2 corresponding encoder layer (skip connection)\n",
    "        x1 = self.up(x1)\n",
    "        \n",
    "        # calculate the difference in size between x1 (upsampled) and x2 (skip connection)\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "        \n",
    "        # pad x1 to match the size of x2\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        \n",
    "        # skip connection\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        # 1x1 convolution to map feature channels to the number of classes\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module): \n",
    "    #full unet\n",
    "    def __init__(self, n_channels, n_classes):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        # define encoder sizes\n",
    "        self.inc = DoubleConv(n_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        self.down4 = Down(512, 1024)\n",
    "        # define decoder sizes\n",
    "        self.up1 = Up(1024, 512)\n",
    "        self.up2 = Up(512, 256)\n",
    "        self.up3 = Up(256, 128)\n",
    "        self.up4 = Up(128, 64)\n",
    "        self.outc = OutConv(64, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # encoder \n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        \n",
    "        # decoder - with skip connections\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "\n",
    "        # final\n",
    "        out = self.outc(x) \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    # dataset\n",
    "    json_files = ['label_data_0313.json', 'label_data_0531.json', 'label_data_0601.json']\n",
    "    \n",
    "    full_dataset = TuSimpleDataset(TRAIN_SET_DIR, PROCESSED_DATA_DIR, json_files)\n",
    "    \n",
    "    # split into train and val\n",
    "    train_size = int(0.9 * len(full_dataset))\n",
    "    val_size = len(full_dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "    \n",
    "    # model\n",
    "    model = UNet(n_channels=3, n_classes=NUM_CLASSES).to(device) # 3 channels for R,G,B \n",
    "    \n",
    "    # loss and optimizer (instance seg -> adding other lane weights)\n",
    "    class_weights = torch.tensor([0.4, 1.0, 1.0, 1.0, 1.0, 1.0], device=device) # background, lanes\n",
    "    criterion_ce = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    \n",
    "    best_val_loss = float('inf') # checkpoint\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\") \n",
    "        \n",
    "        for images, masks in loop:\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            \n",
    "            # forward pass\n",
    "            outputs = model(images)\n",
    "            \n",
    "            loss_ce = criterion_ce(outputs, masks)\n",
    "            loss = loss_ce\n",
    "            \n",
    "            # backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        \n",
    "        # validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        conf_matrix = np.zeros((NUM_CLASSES, NUM_CLASSES), dtype=np.int64)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, masks in val_loader:\n",
    "                images = images.to(device)\n",
    "                masks = masks.to(device)\n",
    "                outputs = model(images)\n",
    "                \n",
    "                loss_ce = criterion_ce(outputs, masks)\n",
    "                loss = loss_ce\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                probs = torch.softmax(outputs, dim=1)\n",
    "                preds = torch.argmax(probs, dim=1)\n",
    "                \n",
    "                preds_flat = preds.view(-1).cpu().numpy()\n",
    "                masks_flat = masks.view(-1).cpu().numpy()\n",
    "                \n",
    "                bins = NUM_CLASSES * masks_flat + preds_flat\n",
    "                bincount = np.bincount(bins, minlength=NUM_CLASSES**2)\n",
    "                conf_matrix += bincount.reshape(NUM_CLASSES, NUM_CLASSES)\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{EPOCHS}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "        \n",
    "        # Save Confusion Matrix\n",
    "        cm_filename = f\"confusion_matrix_e{EPOCHS}.npy\"\n",
    "        cm_path = os.path.join(CHECKPOINT_DIR, cm_filename)\n",
    "        np.save(cm_path, conf_matrix)\n",
    "        \n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), BEST_MODEL_PATH)\n",
    "            print(\"Saved best model checkpoint.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Inference & Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lane_detection_prediction(image_path, model_path, output_path):\n",
    "    # load model\n",
    "    model = UNet(n_channels=3, n_classes=NUM_CLASSES).to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    # load image\n",
    "    example_image = cv2.imread(image_path)\n",
    "    \n",
    "    image = cv2.cvtColor(example_image, cv2.COLOR_BGR2RGB) #convert to RGB\n",
    "    image = cv2.resize(image, (IMG_WIDTH, IMG_HEIGHT)) #resize to match input size\n",
    "    image = image.astype(np.float32) / 255.0 #normalize\n",
    "    image = np.transpose(image, (2, 0, 1)) #channels first\n",
    "    image = torch.from_numpy(image).float().unsqueeze(0).to(device) \n",
    "\n",
    "    # inference\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        probs = torch.softmax(output, dim=1)\n",
    "        pred_mask = torch.argmax(probs, dim=1).squeeze(0).cpu().numpy()\n",
    "    \n",
    "    # resize to Original Size\n",
    "    pred_mask_resized = cv2.resize(pred_mask.astype(np.uint8), (example_image.shape[1], example_image.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "    \n",
    "    colors = [\n",
    "        [0, 0, 0],       # Background\n",
    "        [0, 255, 0],     # Lane 1 (Green)\n",
    "        [255, 0, 0],     # Lane 2 (Red)\n",
    "        [0, 0, 255],     # Lane 3 (Blue)\n",
    "        [255, 255, 0],   # Lane 4 (Yellow)\n",
    "        [255, 0, 255]    # Lane 5 (Magenta)\n",
    "    ]\n",
    "    \n",
    "    overlay = np.zeros_like(example_image)\n",
    "    \n",
    "    for i in range(1, NUM_CLASSES):\n",
    "        class_mask = (pred_mask_resized == i).astype(np.uint8)\n",
    "        \n",
    "        #connected components directly on class mask\n",
    "        num_labels, labels_im, stats, centroids = cv2.connectedComponentsWithStats(class_mask)\n",
    "        \n",
    "        for j in range(1, num_labels):\n",
    "            area = stats[j, cv2.CC_STAT_AREA]\n",
    "            height = stats[j, cv2.CC_STAT_HEIGHT]\n",
    "            \n",
    "            #relaxed filtering: area > 250, height > 20\n",
    "            if area >= 250 and height >= 20:\n",
    "                valid_mask = (labels_im == j)\n",
    "                overlay[valid_mask] = colors[i]\n",
    "\n",
    "    result = cv2.addWeighted(example_image, 1, overlay, 0.5, 0) \n",
    "\n",
    "    cv2.imwrite(output_path, result)\n",
    "    print(f\"Saved result to {output_path}\")\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(cv2.cvtColor(result, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Execution\n",
    "Execution of the training functions and inference. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run Preprocessing\n",
    "json_files = ['label_data_0313.json', 'label_data_0531.json', 'label_data_0601.json']\n",
    "process_tusimple_data(TRAIN_SET_DIR, PROCESSED_DATA_DIR, json_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train Model\n",
    "#train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference\n",
    "test_image_path1 = os.path.join(TRAIN_SET_DIR, 'clips/0313-2/50/10.jpg') # Example image\n",
    "test_image_path2 = os.path.join(TRAIN_SET_DIR, 'clips/0313-1/1800/10.jpg') \n",
    "test_image_path3 = os.path.join(TRAIN_SET_DIR, 'clips/0601/1494452553518276564/20.jpg') \n",
    "lane_detection_prediction(test_image_path1, BEST_MODEL_PATH, 'instance_result.jpg')\n",
    "lane_detection_prediction(test_image_path2, BEST_MODEL_PATH, 'instance_result2.jpg')\n",
    "lane_detection_prediction(test_image_path3, BEST_MODEL_PATH, 'instance_result3.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluation\n",
    "Calculate IoU and Accuracy on the Test Set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tolerance_iou(pred_mask, true_mask, num_classes, tolerance=5):\n",
    "    # Tolerance IoU: Dilate GT\n",
    "    iou_per_class = []\n",
    "    kernel = np.ones((tolerance, tolerance), np.uint8)\n",
    "    \n",
    "    for cls in range(num_classes):\n",
    "        if cls == 0: \n",
    "            # standard IoU\n",
    "            p = (pred_mask == cls)\n",
    "            t = (true_mask == cls)\n",
    "            inter = np.logical_and(p, t).sum()\n",
    "            union = np.logical_or(p, t).sum()\n",
    "            iou_per_class.append(inter / union if union > 0 else np.nan)\n",
    "            continue\n",
    "\n",
    "        pred_cls = (pred_mask == cls).astype(np.uint8)\n",
    "        true_cls = (true_mask == cls).astype(np.uint8)\n",
    "        \n",
    "        # dilate ground truth\n",
    "        dilated_true = cv2.dilate(true_cls, kernel, iterations=1)\n",
    "        \n",
    "        intersection = np.logical_and(pred_cls, dilated_true).sum()\n",
    "        union = np.logical_or(pred_cls, true_cls).sum()\n",
    "        \n",
    "        if union == 0:\n",
    "            iou_per_class.append(np.nan)\n",
    "        else:\n",
    "            iou_per_class.append(intersection / union)\n",
    "            \n",
    "    return iou_per_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader, device, val_cm_path=None):\n",
    "    num_visualize = 3 \n",
    "    model.eval()\n",
    "    \n",
    "    # Load Validation Metrics\n",
    "    val_cm = np.load(val_cm_path)\n",
    "    \n",
    "    # Calculate Metrics from Val CM\n",
    "    v_inter = np.diag(val_cm)\n",
    "    v_union = val_cm.sum(axis=1) + val_cm.sum(axis=0) - v_inter\n",
    "    val_iou = v_inter / np.maximum(v_union, 1)\n",
    "    \n",
    "    v_tp = v_inter\n",
    "    v_fp = val_cm.sum(axis=0) - v_tp\n",
    "    v_fn = val_cm.sum(axis=1) - v_tp\n",
    "    val_f1 = 2 * v_tp / np.maximum(2 * v_tp + v_fp + v_fn, 1)\n",
    "    \n",
    "    # Calculate Test Metrics\n",
    "    test_cm = np.zeros((NUM_CLASSES, NUM_CLASSES), dtype=np.int64)\n",
    "    total_tol_iou = []\n",
    "    viz_samples = []\n",
    "    \n",
    "    colors = [[0, 0, 0], [0, 255, 0], [255, 0, 0], [0, 0, 255], [255, 255, 0], [255, 0, 255]]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, masks in tqdm(test_loader, desc=\"Evaluating Test Set\"):\n",
    "            images = images.to(device)\n",
    "            masks_np = masks.cpu().numpy()\n",
    "            masks = masks.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            # Initial Argmax\n",
    "            raw_preds = torch.argmax(probs, dim=1).cpu().numpy()\n",
    "            \n",
    "            # Process Each Image in Batch to Apply Filtering\n",
    "            processed_preds = np.zeros_like(raw_preds)\n",
    "            \n",
    "            for b in range(len(raw_preds)):\n",
    "                img_pred = raw_preds[b]\n",
    "                \n",
    "                # Apply Filtering per class\n",
    "                for cls in range(1, NUM_CLASSES): #skip BG for filtering\n",
    "                    class_mask = (img_pred == cls).astype(np.uint8)\n",
    "                    \n",
    "                    num_labels, labels_im, stats, _ = cv2.connectedComponentsWithStats(class_mask)\n",
    "                    \n",
    "                    for j in range(1, num_labels):\n",
    "                        area = stats[j, cv2.CC_STAT_AREA]\n",
    "                        height = stats[j, cv2.CC_STAT_HEIGHT]\n",
    "                        if area >= 250 and height >= 20:\n",
    "                             processed_preds[b][labels_im == j] = cls\n",
    "            \n",
    "            # Update Test CM & Tolerance IoU using PROCESSED predictions\n",
    "            for i in range(len(masks_np)):\n",
    "                # CM\n",
    "                p_flat = processed_preds[i].flatten()\n",
    "                t_flat = masks_np[i].flatten()\n",
    "                bins = NUM_CLASSES * t_flat + p_flat\n",
    "                bincount = np.bincount(bins, minlength=NUM_CLASSES**2)\n",
    "                test_cm += bincount.reshape(NUM_CLASSES, NUM_CLASSES)\n",
    "                \n",
    "                # Tolerance IoU\n",
    "                tol_iou = calculate_tolerance_iou(processed_preds[i], masks_np[i], NUM_CLASSES, tolerance=5)\n",
    "                total_tol_iou.append(tol_iou)\n",
    "\n",
    "                if len(viz_samples) < num_visualize:\n",
    "                    img_np = images[i].cpu().permute(1, 2, 0).numpy()\n",
    "                    img_np = (img_np * 255).astype(np.uint8)\n",
    "                    img_np_rgb = cv2.cvtColor(img_np, cv2.COLOR_BGR2RGB)\n",
    "                    viz_samples.append((img_np_rgb, masks_np[i], processed_preds[i]))\n",
    "    \n",
    "    # Calculate Test Metrics\n",
    "    t_inter = np.diag(test_cm)\n",
    "    t_union = test_cm.sum(axis=1) + test_cm.sum(axis=0) - t_inter\n",
    "    test_iou = t_inter / np.maximum(t_union, 1)\n",
    "    \n",
    "    t_tp = t_inter\n",
    "    t_fp = test_cm.sum(axis=0) - t_tp\n",
    "    t_fn = test_cm.sum(axis=1) - t_tp\n",
    "    test_f1 = 2 * t_tp / np.maximum(2 * t_tp + t_fp + t_fn, 1)\n",
    "    \n",
    "    mean_tol_iou = np.nanmean(total_tol_iou, axis=0)\n",
    "    \n",
    "    # Simple List Output\n",
    "    print(f\"\\nValidation Mean IoU: {np.nanmean(val_iou[1:]):.4f}\")\n",
    "    print(f\"Validation Mean F1:  {np.nanmean(val_f1[1:]):.4f}\")\n",
    "    print(f\"Test Mean IoU:       {np.nanmean(test_iou[1:]):.4f}\")\n",
    "    print(f\"Test Mean F1:        {np.nanmean(test_f1[1:]):.4f}\")\n",
    "    print(f\"Test Mean Tol IoU:   {np.nanmean(mean_tol_iou[1:]):.4f}\\n\")\n",
    "    \n",
    "    for i in range(1, NUM_CLASSES):\n",
    "        print(f\"Lane {i}: Val IoU: {val_iou[i]:.4f}, Test IoU: {test_iou[i]:.4f}, Test Tol IoU: {mean_tol_iou[i]:.4f}, Val F1: {val_f1[i]:.4f}, Test F1: {test_f1[i]:.4f}\")\n",
    "\n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(num_visualize, 3, figsize=(15, 5 * num_visualize))\n",
    "    for idx, (img, gt, pred) in enumerate(viz_samples):\n",
    "        current_axes = axes[idx]\n",
    "        current_axes[0].imshow(img)\n",
    "        current_axes[0].set_title(\"Input\")\n",
    "        current_axes[0].axis('off')\n",
    "        \n",
    "        ov_gt = img.copy()\n",
    "        ov_pred = img.copy()\n",
    "        for c in range(1, NUM_CLASSES):\n",
    "             ov_gt[gt==c] = colors[c]\n",
    "             ov_pred[pred==c] = colors[c]\n",
    "             \n",
    "        current_axes[1].imshow(ov_gt)\n",
    "        current_axes[1].set_title(\"Ground Truth\")\n",
    "        current_axes[1].axis('off')\n",
    "        \n",
    "        current_axes[2].imshow(ov_pred)\n",
    "        current_axes[2].set_title(f\"Pred (Filtered)\")\n",
    "        current_axes[2].axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_json_files = ['test_label.json']\n",
    "process_tusimple_data(TEST_SET_DIR, PROCESSED_DATA_DIR, test_json_files)\n",
    "\n",
    "test_dataset = TuSimpleDataset(TEST_SET_DIR, PROCESSED_DATA_DIR, test_json_files)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "model = UNet(n_channels=3, n_classes=NUM_CLASSES).to(device)\n",
    "model.load_state_dict(torch.load(EVAL_MODEL_PATH, map_location=device))\n",
    "\n",
    "val_cm_path = os.path.join(CHECKPOINT_DIR, f\"confusion_matrix_e{EPOCHS}.npy\")\n",
    "\n",
    "evaluate(model, test_loader, device, val_cm_path=val_cm_path, plotmode=False, num_visualize=5)\n",
    "evaluate(model, test_loader, device, val_cm_path=val_cm_path, plotmode=True, num_visualize=30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (ADAS-Project)",
   "language": "python",
   "name": "venv_adas_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
