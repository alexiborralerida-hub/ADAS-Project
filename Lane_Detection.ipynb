{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Lane Detection with TuSimple Dataset\n",
                "Using U-Net\n",
                "1. Configuration\n",
                "2. Data Preprocessing (JSON to Masks)\n",
                "3. Dataset Loading\n",
                "4. Model Architecture (U-Net)\n",
                "5. Training Loop\n",
                "6. Inference & Visualization\n",
                "7. Execution\n",
                "8. Evaluation\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import json\n",
                "import cv2\n",
                "import numpy as np\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import Dataset, DataLoader, random_split\n",
                "from tqdm import tqdm\n",
                "import matplotlib.pyplot as plt\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Configuration\n",
                "Define paths, hyperparameters, and device settings.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Using device: cuda\n"
                    ]
                }
            ],
            "source": [
                "# Paths\n",
                "BASE_DIR = os.getcwd()\n",
                "DATA_DIR = r'C:\\ADAS_Project\\TUSimple_Small' \n",
                "TRAIN_SET_DIR = os.path.join(DATA_DIR, 'train_set')\n",
                "TEST_SET_DIR = os.path.join(DATA_DIR, 'test_set')\n",
                "PROCESSED_DATA_DIR = os.path.join(DATA_DIR, 'processed')\n",
                "CHECKPOINT_DIR = os.path.join(BASE_DIR, 'Lane_Detection', 'checkpoints')\n",
                "\n",
                "# Create directories\n",
                "os.makedirs(PROCESSED_DATA_DIR, exist_ok=True)\n",
                "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
                "BEST_MODEL_PATH = os.path.join(CHECKPOINT_DIR, 'best_model.pth')\n",
                "\n",
                "# Hyperparameters\n",
                "EPOCHS = 5 #20+ for final training\n",
                "BATCH_SIZE = 4 #8+ for final training\n",
                "LEARNING_RATE = 5*1e-4 \n",
                "WEIGHT_DECAY = 1e-4 \n",
                "NUM_CLASSES = 6  # Background + 5 lanes\n",
                "IMG_HEIGHT = 288 # Resize target height\n",
                "IMG_WIDTH = 512  # Resize target width\n",
                "\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"Using device: {device}\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Data Preprocessing\n",
                "Functions to parse TuSimple JSON labels and generate segmentation masks.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def process_tusimple_data(data_dir, output_dir, json_files):\n",
                "    \n",
                "    if not os.path.exists(output_dir):\n",
                "        os.makedirs(output_dir)\n",
                "\n",
                "    for json_file in json_files:\n",
                "        json_path = os.path.join(data_dir, json_file)\n",
                "\n",
                "        with open(json_path, 'r') as openfile: #read json file \n",
                "            lines = openfile.readlines()\n",
                "\n",
                "        for line in tqdm(lines, desc=f\"Processing {json_file}\"):\n",
                "            info = json.loads(line)\n",
                "            raw_file = info['raw_file']\n",
                "            lanes = info['lanes']\n",
                "            h_samples = info['h_samples']\n",
                "            \n",
                "            # Create mask (assuming 1280x720 original size)\n",
                "            mask = np.zeros((720, 1280), dtype=np.uint8)\n",
                "\n",
                "            # Filter out empty lanes\n",
                "            valid_lanes = []\n",
                "            for lane in lanes:\n",
                "                if any(x != -2 for x in lane):\n",
                "                    valid_lanes.append(lane)\n",
                "            \n",
                "            # Sort lanes left-to-right based on average x\n",
                "            lane_centers = []\n",
                "            for lane in valid_lanes:\n",
                "                valid_x = [x for x in lane if x != -2]\n",
                "                if valid_x:\n",
                "                    lane_centers.append(np.mean(valid_x))\n",
                "                else:\n",
                "                    lane_centers.append(0)\n",
                "            \n",
                "            sorted_indices = np.argsort(lane_centers)\n",
                "\n",
                "            for i, idx in enumerate(sorted_indices):\n",
                "                lane = valid_lanes[idx]\n",
                "                lane_id = i + 1 # 1-based ID\n",
                "                if lane_id >= NUM_CLASSES:\n",
                "                    break \n",
                "\n",
                "                # Draw lane on mask\n",
                "                points = []\n",
                "                for x, y in zip(lane, h_samples):\n",
                "                    if x != -2:\n",
                "                        points.append((x, y))\n",
                "                \n",
                "                if len(points) > 1:\n",
                "                    cv2.polylines(mask, [np.array(points, dtype=np.int32)], isClosed=False, color=lane_id, thickness=10)\n",
                "\n",
                "            # Save mask\n",
                "            mask_rel_path = raw_file.replace('.jpg', '.png')\n",
                "            mask_save_path = os.path.join(output_dir, mask_rel_path)\n",
                "            \n",
                "            os.makedirs(os.path.dirname(mask_save_path), exist_ok=True)\n",
                "            cv2.imwrite(mask_save_path, mask)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Dataset Definition\n",
                "PyTorch Dataset class to load images and masks.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "class TuSimpleDataset(Dataset):\n",
                "    def __init__(self, root_dir, processed_dir, json_files, transform=None):\n",
                "        self.root_dir = root_dir\n",
                "        self.processed_dir = processed_dir\n",
                "        self.transform = transform\n",
                "        self.samples = []\n",
                "\n",
                "        # Load all samples from json files\n",
                "        for json_file in json_files:\n",
                "            json_path = os.path.join(root_dir, json_file)\n",
                "            if not os.path.exists(json_path):\n",
                "                continue\n",
                "                \n",
                "            with open(json_path, 'r') as f:\n",
                "                lines = f.readlines()\n",
                "            \n",
                "            for line in lines:\n",
                "                info = json.loads(line)\n",
                "                raw_file = info['raw_file']\n",
                "                mask_file = raw_file.replace('.jpg', '.png')\n",
                "                \n",
                "                # Check if mask exists\n",
                "                if os.path.exists(os.path.join(processed_dir, mask_file)):\n",
                "                    self.samples.append((raw_file, mask_file))\n",
                "\n",
                "    def __len__(self):\n",
                "        return len(self.samples)\n",
                "\n",
                "    def __getitem__(self, idx):\n",
                "        img_rel_path, mask_rel_path = self.samples[idx]\n",
                "        \n",
                "        img_path = os.path.join(self.root_dir, img_rel_path)\n",
                "        mask_path = os.path.join(self.processed_dir, mask_rel_path)\n",
                "        \n",
                "        # Load image\n",
                "        image = cv2.imread(img_path)\n",
                "        if image is None:\n",
                "             raise FileNotFoundError(f\"Image not found: {img_path}\")\n",
                "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
                "        \n",
                "        # Load mask\n",
                "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
                "        if mask is None:\n",
                "             raise FileNotFoundError(f\"Mask not found: {mask_path}\")\n",
                "        \n",
                "        # Resize\n",
                "        image = cv2.resize(image, (IMG_WIDTH, IMG_HEIGHT))\n",
                "        mask = cv2.resize(mask, (IMG_WIDTH, IMG_HEIGHT), interpolation=cv2.INTER_NEAREST)\n",
                "        \n",
                "        # Normalize image\n",
                "        image = image.astype(np.float32) / 255.0\n",
                "        image = np.transpose(image, (2, 0, 1)) # HWC -> CHW\n",
                "        \n",
                "        # Convert to tensor\n",
                "        image = torch.from_numpy(image).float()\n",
                "        mask = torch.from_numpy(mask).long()\n",
                "        \n",
                "        return image, mask\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Model Architecture (U-Net)\n",
                "Standard U-Net implementation.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class DoubleConv(nn.Module):\n",
                "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
                "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
                "        super().__init__()\n",
                "        if not mid_channels:\n",
                "            mid_channels = out_channels\n",
                "        # Two consecutive convolutional layers with Batch Normalization and ReLU activation\n",
                "        # padding=1\n",
                "        self.double_conv = nn.Sequential(\n",
                "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
                "            nn.BatchNorm2d(mid_channels),\n",
                "            nn.ReLU(inplace=True),\n",
                "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
                "            nn.BatchNorm2d(out_channels),\n",
                "            nn.ReLU(inplace=True)\n",
                "        )\n",
                "\n",
                "    def forward(self, x):\n",
                "        return self.double_conv(x)\n",
                "\n",
                "class Down(nn.Module):\n",
                "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
                "    def __init__(self, in_channels, out_channels):\n",
                "        super().__init__()\n",
                "        # MaxPool2d(2) reduces the spatial dimensions by half (e.g., 256x256 -> 128x128)\n",
                "        self.maxpool_conv = nn.Sequential(\n",
                "            nn.MaxPool2d(2),\n",
                "            DoubleConv(in_channels, out_channels)\n",
                "        )\n",
                "\n",
                "    def forward(self, x):\n",
                "        return self.maxpool_conv(x)\n",
                "\n",
                "class Up(nn.Module):\n",
                "    \"\"\"Upscaling then double conv\"\"\"\n",
                "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
                "        super().__init__()\n",
                "\n",
                "        # Upsample the feature map to increase spatial dimensions\n",
                "        if bilinear:\n",
                "            # Bilinear interpolation for upsampling (no learnable parameters)\n",
                "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
                "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
                "        else:\n",
                "            # Transposed convolution for upsampling (learnable parameters)\n",
                "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
                "            self.conv = DoubleConv(in_channels, out_channels)\n",
                "\n",
                "    def forward(self, x1, x2):\n",
                "        # x1: Feature map from the previous layer (to be upsampled)\n",
                "        # x2: Feature map from the corresponding encoder layer (skip connection)\n",
                "        x1 = self.up(x1)\n",
                "        \n",
                "        # Calculate the difference in size between x1 (upsampled) and x2 (skip connection)\n",
                "        # This handles cases where input dimensions are not perfectly divisible by 2^depth\n",
                "        diffY = x2.size()[2] - x1.size()[2]\n",
                "        diffX = x2.size()[3] - x1.size()[3]\n",
                "\n",
                "        # Pad x1 to match the size of x2\n",
                "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
                "                        diffY // 2, diffY - diffY // 2])\n",
                "        \n",
                "        # Concatenate along the channel dimension (dim=1)\n",
                "        # This is the \"Skip Connection\" that re-introduces spatial information\n",
                "        x = torch.cat([x2, x1], dim=1)\n",
                "        return self.conv(x)\n",
                "\n",
                "class OutConv(nn.Module):\n",
                "    def __init__(self, in_channels, out_channels):\n",
                "        super(OutConv, self).__init__()\n",
                "        # 1x1 convolution to map feature channels to the number of classes\n",
                "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
                "\n",
                "    def forward(self, x):\n",
                "        return self.conv(x)\n",
                "\n",
                "class UNet(nn.Module):\n",
                "    def __init__(self, n_channels, n_classes, bilinear=True):\n",
                "        super(UNet, self).__init__()\n",
                "        self.n_channels = n_channels\n",
                "        self.n_classes = n_classes\n",
                "        self.bilinear = bilinear\n",
                "\n",
                "        # Encoder (Contracting Path)\n",
                "        self.inc = DoubleConv(n_channels, 64)\n",
                "        self.down1 = Down(64, 128)\n",
                "        self.down2 = Down(128, 256)\n",
                "        self.down3 = Down(256, 512)\n",
                "        factor = 2 if bilinear else 1\n",
                "        self.down4 = Down(512, 1024 // factor)\n",
                "        self.up1 = Up(1024, 512 // factor, bilinear)\n",
                "        self.up2 = Up(512, 256 // factor, bilinear)\n",
                "        self.up3 = Up(256, 128 // factor, bilinear)\n",
                "        self.up4 = Up(128, 64, bilinear)\n",
                "        self.outc = OutConv(64, n_classes)\n",
                "\n",
                "    def forward(self, x):\n",
                "        # Encoder: Capture context\n",
                "        x1 = self.inc(x)\n",
                "        x2 = self.down1(x1)\n",
                "        x3 = self.down2(x2)\n",
                "        x4 = self.down3(x3)\n",
                "        x5 = self.down4(x4)\n",
                "        \n",
                "        # Decoder: Precise localization using skip connections\n",
                "        x = self.up1(x5, x4)\n",
                "        x = self.up2(x, x3)\n",
                "        x = self.up3(x, x2)\n",
                "        x = self.up4(x, x1)\n",
                "        \n",
                "        # Final classification\n",
                "        logits = self.outc(x)\n",
                "        return logits"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Training Loop\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_model():\n",
                "    # Dataset\n",
                "    json_files = ['label_data_0313.json', 'label_data_0531.json', 'label_data_0601.json']\n",
                "    \n",
                "    full_dataset = TuSimpleDataset(TRAIN_SET_DIR, PROCESSED_DATA_DIR, json_files)\n",
                "\n",
                "    # Split into train and val\n",
                "    train_size = int(0.9 * len(full_dataset))\n",
                "    val_size = len(full_dataset) - train_size\n",
                "    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
                "\n",
                "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
                "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
                "\n",
                "    # Model\n",
                "    model = UNet(n_channels=3, n_classes=NUM_CLASSES).to(device) #channels for R,G,B \n",
                "\n",
                "    # Loss and optimizer\n",
                "    class_weights = torch.tensor([0.05, 1.0, 1.0, 1.0, 1.0, 1.0], device=device) #background class weight is 0.05.\n",
                "    criterion_ce = nn.CrossEntropyLoss(weight=class_weights)\n",
                "    \n",
                "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
                "\n",
                "    # Training loop\n",
                "\n",
                "    best_val_loss = float('inf') # Checkpoint: set the best val to infinity at first\n",
                "\n",
                "    for epoch in range(EPOCHS):\n",
                "        model.train()\n",
                "        train_loss = 0\n",
                "        \n",
                "        loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
                "        for images, masks in loop:\n",
                "            images = images.to(device)\n",
                "            masks = masks.to(device)\n",
                "\n",
                "            # Forward pass\n",
                "            outputs = model(images)\n",
                "            \n",
                "            # CE Loss\n",
                "            loss_ce = criterion_ce(outputs, masks)\n",
                "            loss = loss_ce\n",
                "\n",
                "            # Backward and optimize\n",
                "            optimizer.zero_grad()\n",
                "            loss.backward()\n",
                "            optimizer.step()\n",
                "\n",
                "            train_loss += loss.item()\n",
                "            loop.set_postfix(loss=loss.item(), ce=loss_ce.item())\n",
                "\n",
                "        avg_train_loss = train_loss / len(train_loader)\n",
                "        \n",
                "        # Validation, no_grad for validation\n",
                "        model.eval()\n",
                "        val_loss = 0\n",
                "        with torch.no_grad():\n",
                "            for images, masks in val_loader:\n",
                "                images = images.to(device)\n",
                "                masks = masks.to(device)\n",
                "                outputs = model(images)\n",
                "                \n",
                "                loss_ce = criterion_ce(outputs, masks)\n",
                "                loss = loss_ce\n",
                "                \n",
                "                val_loss += loss.item()\n",
                "        \n",
                "        avg_val_loss = val_loss / len(val_loader)\n",
                "        print(f\"Epoch [{epoch+1}/{EPOCHS}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
                "\n",
                "        # Save best model\n",
                "        if avg_val_loss < best_val_loss:\n",
                "            best_val_loss = avg_val_loss\n",
                "            torch.save(model.state_dict(), BEST_MODEL_PATH)\n",
                "            print(\"Saved best model checkpoint.\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Inference & Visualization\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "def lane_detection_prediction(image_path, model_path, output_path):\n",
                "    # Load model\n",
                "    model = UNet(n_channels=3, n_classes=NUM_CLASSES).to(device)\n",
                "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
                "    model.eval()\n",
                "\n",
                "    # Load image\n",
                "    original_image = cv2.imread(image_path)\n",
                "\n",
                "    image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
                "    image = cv2.resize(image, (IMG_WIDTH, IMG_HEIGHT))\n",
                "    image = image.astype(np.float32) / 255.0\n",
                "    image = np.transpose(image, (2, 0, 1))\n",
                "    image = torch.from_numpy(image).float().unsqueeze(0).to(device)\n",
                "\n",
                "    # Inference\n",
                "    with torch.no_grad():\n",
                "        output = model(image)\n",
                "        probs = torch.softmax(output, dim=1)\n",
                "        pred_mask = torch.argmax(probs, dim=1).squeeze(0).cpu().numpy()\n",
                "\n",
                "    # Visualization\n",
                "    pred_mask_resized = cv2.resize(pred_mask.astype(np.uint8), (original_image.shape[1], original_image.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
                "    \n",
                "    colors = [\n",
                "        [0, 0, 0],       # Background\n",
                "        [255, 0, 0],     # Lane 1 (Blue)\n",
                "        [0, 255, 0],     # Lane 2 (Green)\n",
                "        [0, 0, 255],     # Lane 3 (Red)\n",
                "        [255, 255, 0],   # Lane 4 (Cyan)\n",
                "        [255, 0, 255]    # Lane 5 (Magenta)\n",
                "    ]\n",
                "    \n",
                "    overlay = np.zeros_like(original_image)\n",
                "    for i in range(1, NUM_CLASSES):\n",
                "        overlay[pred_mask_resized == i] = colors[i]\n",
                "\n",
                "    alpha = 0.5\n",
                "    result = cv2.addWeighted(original_image, 1, overlay, alpha, 0)\n",
                "\n",
                "    cv2.imwrite(output_path, result)\n",
                "    print(f\"Saved result to {output_path}\")\n",
                "    \n",
                "    plt.figure(figsize=(10, 5))\n",
                "    plt.imshow(cv2.cvtColor(result, cv2.COLOR_BGR2RGB))\n",
                "    plt.axis('off')\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Execution\n",
                "Run the following cells to process data and train the model.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Processing label_data_0313.json: 100%|██████████| 708/708 [00:03<00:00, 192.83it/s]\n",
                        "Processing label_data_0531.json: 100%|██████████| 88/88 [00:00<00:00, 201.50it/s]\n",
                        "Processing label_data_0601.json: 100%|██████████| 110/110 [00:00<00:00, 192.47it/s]\n"
                    ]
                }
            ],
            "source": [
                "# 1. Run Preprocessing\n",
                "json_files = ['label_data_0313.json', 'label_data_0531.json', 'label_data_0601.json']\n",
                "process_tusimple_data(TRAIN_SET_DIR, PROCESSED_DATA_DIR, json_files)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 1/20: 100%|██████████| 102/102 [26:21<00:00, 15.50s/it, ce=0.587, loss=0.587]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch [1/20], Train Loss: 0.9532, Val Loss: 0.7134\n",
                        "Saved best model checkpoint.\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 2/20:   3%|▎         | 3/102 [00:56<31:02, 18.82s/it, ce=0.585, loss=0.585]\n"
                    ]
                },
                {
                    "ename": "KeyboardInterrupt",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
                        "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 2. Train Model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
                        "Cell \u001b[1;32mIn[6], line 49\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m     46\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     47\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 49\u001b[0m     train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m     loop\u001b[38;5;241m.\u001b[39mset_postfix(loss\u001b[38;5;241m=\u001b[39mloss\u001b[38;5;241m.\u001b[39mitem(), ce\u001b[38;5;241m=\u001b[39mloss_ce\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m     52\u001b[0m avg_train_loss \u001b[38;5;241m=\u001b[39m train_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n",
                        "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
                    ]
                }
            ],
            "source": [
                "# 2. Train Model\n",
                "train_model()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. Inference\n",
                "test_image_path = os.path.join(TRAIN_SET_DIR, 'clips/0313-1/60/20.jpg') # Example image\n",
                "output_image_path = 'inference_result.jpg'\n",
                "lane_detection_prediction(test_image_path, BEST_MODEL_PATH, output_image_path)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Evaluation\n",
                "Calculate IoU, Dice Score, and Accuracy on the Test Set.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def calculate_metrics(pred_mask, true_mask, num_classes):\n",
                "    # pred_mask, true_mask: [H, W]\n",
                "    iou_per_class = []\n",
                "    dice_per_class = []\n",
                "    \n",
                "    for cls in range(num_classes):\n",
                "        pred_cls = (pred_mask == cls)\n",
                "        true_cls = (true_mask == cls)\n",
                "        \n",
                "        intersection = np.logical_and(pred_cls, true_cls).sum()\n",
                "        union = np.logical_or(pred_cls, true_cls).sum()\n",
                "        \n",
                "        if union == 0:\n",
                "            iou = float('nan') # Ignore if class not present\n",
                "            dice = float('nan')\n",
                "        else:\n",
                "            iou = intersection / union\n",
                "            dice = 2 * intersection / (pred_cls.sum() + true_cls.sum())\n",
                "            \n",
                "        iou_per_class.append(iou)\n",
                "        dice_per_class.append(dice)\n",
                "        \n",
                "    accuracy = (pred_mask == true_mask).sum() / pred_mask.size\n",
                "    \n",
                "    return iou_per_class, dice_per_class, accuracy\n",
                "\n",
                "def evaluate(model, test_loader, device):\n",
                "    model.eval()\n",
                "    total_iou = []\n",
                "    total_dice = []\n",
                "    total_acc = []\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        for images, masks in tqdm(test_loader, desc=\"Evaluating\"):\n",
                "            images = images.to(device)\n",
                "            masks = masks.cpu().numpy() # Ground truth\n",
                "            \n",
                "            outputs = model(images)\n",
                "            probs = torch.softmax(outputs, dim=1)\n",
                "            preds = torch.argmax(probs, dim=1).cpu().numpy()\n",
                "            \n",
                "            for i in range(len(masks)):\n",
                "                iou, dice, acc = calculate_metrics(preds[i], masks[i], NUM_CLASSES)\n",
                "                total_iou.append(iou)\n",
                "                total_dice.append(dice)\n",
                "                total_acc.append(acc)\n",
                "                \n",
                "    # Average metrics (ignoring NaNs)\n",
                "    mean_iou = np.nanmean(total_iou, axis=0)\n",
                "    mean_dice = np.nanmean(total_dice, axis=0)\n",
                "    mean_acc = np.mean(total_acc)\n",
                "    \n",
                "    print(f\"Mean Accuracy: {mean_acc:.4f}\")\n",
                "    print(f\"Mean IoU per class: {mean_iou}\")\n",
                "    print(f\"Mean Dice per class: {mean_dice}\")\n",
                "    print(f\"Mean IoU (Lanes): {np.nanmean(mean_iou[1:])}\") # Exclude background\n",
                "    print(f\"Mean Dice (Lanes): {np.nanmean(mean_dice[1:])}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Process Test Set Labels\n",
                "test_json_files = ['test_label.json']\n",
                "process_tusimple_data(TEST_SET_DIR, PROCESSED_DATA_DIR, test_json_files)\n",
                "\n",
                "# 2. Create Test Dataset and Loader\n",
                "test_dataset = TuSimpleDataset(TEST_SET_DIR, PROCESSED_DATA_DIR, test_json_files)\n",
                "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
                "\n",
                "# 3. Run Evaluation\n",
                "model = UNet(n_channels=3, n_classes=NUM_CLASSES).to(device)\n",
                "if os.path.exists(BEST_MODEL_PATH):\n",
                "    model.load_state_dict(torch.load(BEST_MODEL_PATH, map_location=device))\n",
                "    print(\"Loaded best model.\")\n",
                "else:\n",
                "    print(\"Best model not found, using current model weights.\")\n",
                "\n",
                "evaluate(model, test_loader, device)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.8"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
