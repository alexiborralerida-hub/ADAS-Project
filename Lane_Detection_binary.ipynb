{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lane Detection with TuSimple Dataset\n",
    "Using U-Net\n",
    "1. Configuration\n",
    "2. Data Preprocessing (JSON to Masks)\n",
    "3. Dataset Loading\n",
    "4. Model Architecture (U-Net)\n",
    "5. Training Loop\n",
    "6. Inference & Visualization\n",
    "7. Execution\n",
    "8. Evaluation\n",
    "\n",
    "Save the Confusion Matrix while the validation\n",
    "\n",
    "differentiation of lanes: arc loss, distance between lanes. \n",
    "last layer \n",
    "instance segmentation\n",
    "\n",
    "+3 credits: from instance segmentation -> shape of lane markings scipy make_splprep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration\n",
    "Define paths, hyperparameters, and device settings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "BASE_DIR = os.getcwd()\n",
    "DATA_DIR = r'C:\\ADAS_Project\\TUSimple_Small' \n",
    "TRAIN_SET_DIR = os.path.join(DATA_DIR, 'train_set')\n",
    "TEST_SET_DIR = os.path.join(DATA_DIR, 'test_set')\n",
    "PROCESSED_DATA_DIR = os.path.join(DATA_DIR, 'processed')\n",
    "CHECKPOINT_DIR = os.path.join(BASE_DIR, r'checkpoints\\binary')\n",
    "\n",
    "os.makedirs(PROCESSED_DATA_DIR, exist_ok=True)\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "# Hyperparameters\n",
    "EPOCHS = 5 #20+ for final training\n",
    "BATCH_SIZE = 8 # calculate based on GPU memory \n",
    "LEARNING_RATE = 5*1e-4 \n",
    "WEIGHT_DECAY = 1e-4 \n",
    "NUM_CLASSES = 2  # background + all lanes \n",
    "IMG_HEIGHT = 288 # resize target height\n",
    "IMG_WIDTH = 512  # resize target width\n",
    "\n",
    "# save model checkpoints\n",
    "model_filename = f'best_model_binary_e{EPOCHS}.pth'\n",
    "BEST_MODEL_PATH = os.path.join(CHECKPOINT_DIR, model_filename)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing\n",
    "Functions to parse TuSimple JSON labels and generate segmentation masks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tusimple_data(data_dir, output_dir, json_files):\n",
    "    \n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    for json_file in json_files:\n",
    "        json_path = os.path.join(data_dir, json_file)\n",
    "\n",
    "        with open(json_path, 'r') as openfile: #read json file \n",
    "            lines = openfile.readlines()\n",
    "\n",
    "        for line in tqdm(lines, desc=f\"Processing {json_file}\"):\n",
    "            info = json.loads(line)\n",
    "            raw_file = info['raw_file']\n",
    "            lanes = info['lanes']\n",
    "            h_samples = info['h_samples']\n",
    "            \n",
    "            # create mask assuming 1280x720\n",
    "            mask = np.zeros((720, 1280), dtype=np.uint8)\n",
    "\n",
    "            # filter out empty lanes, -2 for no lane\n",
    "            valid_lanes = []\n",
    "            for lane in lanes:\n",
    "                if any(x != -2 for x in lane):\n",
    "                    valid_lanes.append(lane)\n",
    "            \n",
    "            # binary: draw all valid lanes with class ID 1\n",
    "            for lane in valid_lanes:\n",
    "                points = []\n",
    "                for x, y in zip(lane, h_samples):\n",
    "                    if x != -2:\n",
    "                        points.append((x, y))\n",
    "                \n",
    "                if len(points) > 1:\n",
    "                    cv2.polylines(mask, [np.array(points, dtype=np.int32)], isClosed=False, color=1, thickness=10)\n",
    "                    \n",
    "            # save mask\n",
    "            mask_rel_path = raw_file.replace('.jpg', '.png')\n",
    "            mask_save_path = os.path.join(output_dir, mask_rel_path)\n",
    "            \n",
    "            os.makedirs(os.path.dirname(mask_save_path), exist_ok=True)\n",
    "            cv2.imwrite(mask_save_path, mask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Definition\n",
    "PyTorch Dataset class to load images and masks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TuSimpleDataset(Dataset):\n",
    "    def __init__(self, root_dir, processed_dir, json_files, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.processed_dir = processed_dir\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "\n",
    "        # load all samples from json files\n",
    "        for json_file in json_files:\n",
    "            json_path = os.path.join(root_dir, json_file)\n",
    "            if not os.path.exists(json_path):\n",
    "                continue\n",
    "                \n",
    "            with open(json_path, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "            \n",
    "            for line in lines:\n",
    "                info = json.loads(line)\n",
    "                raw_file = info['raw_file']\n",
    "                mask_file = raw_file.replace('.jpg', '.png')\n",
    "                \n",
    "                self.samples.append((raw_file, mask_file))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_rel_path, mask_rel_path = self.samples[idx]\n",
    "        \n",
    "        img_path = os.path.join(self.root_dir, img_rel_path)\n",
    "        mask_path = os.path.join(self.processed_dir, mask_rel_path)\n",
    "        \n",
    "        # load image and mask\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        # resize and normalize\n",
    "        image = cv2.resize(image, (IMG_WIDTH, IMG_HEIGHT))\n",
    "        mask = cv2.resize(mask, (IMG_WIDTH, IMG_HEIGHT), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        image = image.astype(np.float32) / 255.0\n",
    "        image = np.transpose(image, (2, 0, 1)) # channel first\n",
    "      \n",
    "        image = torch.from_numpy(image).float()\n",
    "        mask = torch.from_numpy(mask).long()\n",
    "        \n",
    "        return image, mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Architecture (U-Net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DoubleConv - from original UNet architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    #(convolution => [BN] => ReLU) * 2\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        # padding=1\n",
    "        mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder - downsampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Down(nn.Module):\n",
    "    #downscaling with maxpool then double conv    \n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        # 256x256 -> 128x128\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoder - upsampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Up(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        # upsample  \n",
    "        self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "        self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        # x1 previous layer \n",
    "        # x2 corresponding encoder layer (skip connection)\n",
    "        x1 = self.up(x1)\n",
    "        \n",
    "        # calculate the difference in size between x1 (upsampled) and x2 (skip connection)\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        # Pad x1 to match the size of x2\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        \n",
    "        # skip connection\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        # 1x1 convolution to map feature channels to the number of classes\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module): \n",
    "    #full unet\n",
    "    def __init__(self, n_channels, n_classes):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        # define encoder sizes\n",
    "        self.inc = DoubleConv(n_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        self.down4 = Down(512, 1024)\n",
    "        # define decoder sizes\n",
    "        self.up1 = Up(1024, 512)\n",
    "        self.up2 = Up(512, 256)\n",
    "        self.up3 = Up(256, 128)\n",
    "        self.up4 = Up(128, 64)\n",
    "        self.outc = OutConv(64, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # encoder \n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        \n",
    "        # decoder - with skip connections\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "\n",
    "        # final\n",
    "        out = self.outc(x) \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    # dataset\n",
    "    json_files = ['label_data_0313.json', 'label_data_0531.json', 'label_data_0601.json']\n",
    "    \n",
    "    full_dataset = TuSimpleDataset(TRAIN_SET_DIR, PROCESSED_DATA_DIR, json_files)\n",
    "\n",
    "    # split into train and val\n",
    "    train_size = int(0.9 * len(full_dataset))\n",
    "    val_size = len(full_dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "    # model\n",
    "    model = UNet(n_channels=3, n_classes=NUM_CLASSES).to(device) #3 channels for R,G,B \n",
    "\n",
    "    # loss and optimizer\n",
    "    class_weights = torch.tensor([0.5, 1.0], device=device) # background, Lane\n",
    "    criterion_ce = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "\n",
    "    best_val_loss = float('inf') # checkpoint\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\") #tqdm is a progress bar\n",
    "        \n",
    "        for images, masks in loop:\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "\n",
    "            # forward pass\n",
    "            outputs = model(images)\n",
    "\n",
    "            loss_ce = criterion_ce(outputs, masks)\n",
    "            loss = loss_ce\n",
    "\n",
    "            # backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            loop.set_postfix(loss=loss.item(), ce=loss_ce.item())\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        \n",
    "        # validation, no_grad for validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for images, masks in val_loader:\n",
    "                images = images.to(device)\n",
    "                masks = masks.to(device)\n",
    "                outputs = model(images)\n",
    "                \n",
    "                loss_ce = criterion_ce(outputs, masks)\n",
    "                loss = loss_ce\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{EPOCHS}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        # save best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), BEST_MODEL_PATH)\n",
    "            print(\"Saved best model checkpoint.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Inference & Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lane_detection_prediction(image_path, model_path, output_path):\n",
    "    # load model\n",
    "    model = UNet(n_channels=3, n_classes=NUM_CLASSES).to(device) #3 channels for rgb images\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    # load image\n",
    "    example_image = cv2.imread(image_path)\n",
    "    \n",
    "    image = cv2.cvtColor(example_image, cv2.COLOR_BGR2RGB) #convert to RGB\n",
    "    image = cv2.resize(image, (IMG_WIDTH, IMG_HEIGHT)) #resize to match input size\n",
    "    image = image.astype(np.float32) / 255.0 #normalize\n",
    "    image = np.transpose(image, (2, 0, 1)) #channels first\n",
    "    image = torch.from_numpy(image).float().unsqueeze(0).to(device) \n",
    "\n",
    "    # inference\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        probs = torch.softmax(output, dim=1)\n",
    "        pred_mask = torch.argmax(probs, dim=1).squeeze(0).cpu().numpy()\n",
    "    \n",
    "    # visualization\n",
    "    pred_mask_resized = cv2.resize(pred_mask.astype(np.uint8), (example_image.shape[1], example_image.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "    \n",
    "    #for binary seg -> one lane class \n",
    "    colors = [\n",
    "        [0, 0, 0],       # Background\n",
    "        [0, 255, 0]      # Lane (Green)\n",
    "    ]\n",
    "    \n",
    "    overlay = np.zeros_like(example_image)\n",
    "    for i in range(1, NUM_CLASSES):\n",
    "        overlay[pred_mask_resized == i] = colors[i]\n",
    "\n",
    "    result = cv2.addWeighted(example_image, 1, overlay, 0.5, 0) #overlay mask on original image\n",
    "\n",
    "    cv2.imwrite(output_path, result)\n",
    "    print(f\"Saved result to {output_path}\")\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(cv2.cvtColor(result, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Execution\n",
    "Execution of the training functions and inference. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Run Preprocessing\n",
    "json_files = ['label_data_0313.json', 'label_data_0531.json', 'label_data_0601.json']\n",
    "process_tusimple_data(TRAIN_SET_DIR, PROCESSED_DATA_DIR, json_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Train Model\n",
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Inference\n",
    "test_image_path = os.path.join(TRAIN_SET_DIR, 'clips/0313-2/50/10.jpg') # Example image\n",
    "lane_detection_prediction(test_image_path, BEST_MODEL_PATH, 'binary_result.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluation\n",
    "Calculate IoU and Accuracy on the Test Set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(pred_mask, true_mask, num_classes):\n",
    "    # pred_mask, true_mask: [H, W]\n",
    "    iou_per_class = []\n",
    "    \n",
    "    for classes in range(num_classes): \n",
    "        pred_classes = (pred_mask == classes)\n",
    "        true_classes = (true_mask == classes)\n",
    "        \n",
    "        intersection = np.logical_and(pred_classes, true_classes).sum()\n",
    "        union = np.logical_or(pred_classes, true_classes).sum()\n",
    "        \n",
    "        iou = intersection / union\n",
    "          \n",
    "        iou_per_class.append(iou)\n",
    "        \n",
    "    accuracy = (pred_mask == true_mask).sum() / pred_mask.size\n",
    "    \n",
    "    return iou_per_class, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate(model, test_loader, device, num_visualize=2):\n",
    "    model.eval()\n",
    "    total_iou = []\n",
    "    total_acc = []\n",
    "    \n",
    "    all_ious = []\n",
    "    visualized_count = 0\n",
    "    viz_samples = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, masks in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            images = images.to(device)\n",
    "            masks = masks.cpu().numpy() # Ground truth\n",
    "            \n",
    "            outputs = model(images)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            preds = torch.argmax(probs, dim=1).cpu().numpy()\n",
    "            \n",
    "            for i in range(len(masks)):\n",
    "                iou, acc = calculate_iou(preds[i], masks[i], NUM_CLASSES)\n",
    "                total_iou.append(iou)\n",
    "                total_acc.append(acc)\n",
    "                \n",
    "            # Visualization\n",
    "            if visualized_count < num_visualize:\n",
    "                \n",
    "                img_np = images[0].cpu().permute(1, 2, 0).numpy() # CHW -> HWC\n",
    "                \n",
    "                img_np = (img_np * 255).astype(np.uint8)\n",
    "                img_np = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR) \n",
    "                img_np_rgb = cv2.cvtColor(img_np, cv2.COLOR_BGR2RGB)\n",
    "                \n",
    "                gt_mask_np = masks[0]\n",
    "                pred_mask_np = preds[0]\n",
    "                \n",
    "                # Calculate IoU for the visualization sample\n",
    "                iou_sample, _ = calculate_iou(preds[0], masks[0], NUM_CLASSES)\n",
    "                iou_val = iou_sample[1] if len(iou_sample) > 1 else iou_sample[0]\n",
    "                \n",
    "                viz_samples.append((img_np_rgb, gt_mask_np, pred_mask_np, iou_val))\n",
    "                visualized_count += 1\n",
    "    \n",
    "    mean_iou = np.nanmean(total_iou, axis=0)\n",
    "    mean_acc = np.mean(total_acc)\n",
    "    \n",
    "    # Visualizing Examples\n",
    "    fig, axes = plt.subplots(num_visualize, 3, figsize=(15, 5 * num_visualize))\n",
    "    if num_visualize == 1: axes = [axes] # Handle single case\n",
    "    \n",
    "    for idx, (img, gt, pred, iou) in enumerate(viz_samples):\n",
    "        # Original Image\n",
    "        axes[idx][0].imshow(img)\n",
    "        axes[idx][0].set_title(\"Input Image\")\n",
    "        axes[idx][0].axis('off')\n",
    "        \n",
    "        overlay_gt = img.copy()\n",
    "        overlay_gt[gt == 1] = [0, 255, 0]\n",
    "\n",
    "        axes[idx][1].imshow(overlay_gt)\n",
    "        axes[idx][1].set_title(\"Ground Truth\")\n",
    "        axes[idx][1].axis('off')\n",
    "        \n",
    "        overlay_pred = img.copy()\n",
    "\n",
    "        overlay_pred[pred == 1] = [255, 0, 0]\n",
    "        axes[idx][2].imshow(overlay_pred)\n",
    "        axes[idx][2].set_title(f\"Prediction (IoU: {iou:.3f})\")\n",
    "        axes[idx][2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Mean Accuracy: {mean_acc:.4f}\")\n",
    "    print(f\"Mean IoU for each class: {mean_iou}\")\n",
    "    print(f\"Mean IoU (Lanes): {np.nanmean(mean_iou[1:])}\") # only lanes\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    # Flatten list of lists if needed, but total_iou is list of iou_per_class (lists)\n",
    "    # histogram likely expects scalars.\n",
    "    # We should probably plot Lane IoU histogram.\n",
    "    lane_ious = [x[1] for x in total_iou if len(x) > 1]\n",
    "    if not lane_ious: lane_ious = [x[0] for x in total_iou]\n",
    "    \n",
    "    plt.hist(lane_ious, bins=20, color='skyblue', edgecolor='black')\n",
    "    plt.title('IoU Distribution for Test Set (Lane Class)')\n",
    "    plt.xlabel('IoU Score')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.axvline(np.mean(lane_ious), color='red', linestyle='dashed', linewidth=1, label=f'Mean IoU: {np.mean(lane_ious):.4f}')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_json_files = ['test_label.json']\n",
    "process_tusimple_data(TEST_SET_DIR, PROCESSED_DATA_DIR, test_json_files)\n",
    "\n",
    "test_dataset = TuSimpleDataset(TEST_SET_DIR, PROCESSED_DATA_DIR, test_json_files)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "model = UNet(n_channels=3, n_classes=NUM_CLASSES).to(device) #3 channels for rgb images\n",
    "model.load_state_dict(torch.load(BEST_MODEL_PATH, map_location=device))\n",
    "\n",
    "evaluate(model, test_loader, device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
